{
  "config": {
    "data_path": "UNSW_NB15_training-set.csv",
    "test_path": "UNSW_NB15_testing-set.csv",
    "zero_day_attack": "DoS",
    "input_dim": 32,
    "hidden_dim": 128,
    "embedding_dim": 64,
    "num_clients": 3,
    "num_rounds": 20,
    "local_epochs": 9,
    "learning_rate": 0.001,
    "ethereum_rpc_url": "http://localhost:8545",
    "contract_address": "0x74f2D28CEC2c97186dE1A02C1Bae84D19A7E8BC8",
    "incentive_contract_address": "0x02090bbB57546b0bb224880a3b93D2Ffb0dde144",
    "private_key": "0x4f3edf983ac636a65a842ce7c78d9aa706d3b113bce9c46f30d7d21715b23b1d",
    "aggregator_address": "0x4565f36D8E3cBC1c7187ea39Eb613E484411e075",
    "ipfs_url": "http://localhost:5001",
    "enable_incentives": true,
    "base_reward": 100,
    "max_reward": 1000,
    "min_reputation": 100,
    "device": "cuda",
    "fully_decentralized": false
  },
  "training_history": [
    {
      "round": 1,
      "timestamp": 1760153455.6855702,
      "accuracy": 0.473,
      "client_updates": [
        "ClientUpdate(client_id='client_1', model_parameters={'meta_learner.transductive_net.feature_extractors.0.0.weight': tensor([[-0.1536, -0.1232, -0.1139,  ...,  0.1705, -0.0004, -0.1573],\n        [ 0.0793,  0.0997, -0.0848,  ..., -0.0075, -0.1471,  0.0102],\n        [ 0.0151, -0.0494,  0.0383,  ...,  0.1171, -0.0065, -0.1147],\n        ...,\n        [ 0.1023, -0.0174, -0.0994,  ...,  0.1195,  0.1059,  0.1524],\n        [-0.0191,  0.1556, -0.1675,  ...,  0.0894, -0.0618,  0.1250],\n        [-0.0327,  0.0633,  0.0727,  ...,  0.1209, -0.0277,  0.0136]]), 'meta_learner.transductive_net.feature_extractors.0.0.bias': tensor([ 0.1607, -0.0373, -0.1462,  0.1508, -0.0661,  0.1049,  0.1316,  0.0551,\n        -0.0505, -0.0717,  0.1207,  0.0789, -0.0479, -0.0738, -0.1094, -0.0901,\n        -0.0013,  0.0558, -0.0046,  0.1614,  0.0648,  0.0183,  0.0890, -0.0286,\n         0.0499,  0.0932,  0.0414,  0.0932,  0.0715,  0.0057, -0.0068,  0.1377,\n         0.1069,  0.0017, -0.0321,  0.1039, -0.0209, -0.1680, -0.1123,  0.1294,\n        -0.0426, -0.0229,  0.1442, -0.0133, -0.1399, -0.1222, -0.1147,  0.1404,\n        -0.1265,  0.0240, -0.1195, -0.0926,  0.0386,  0.1258, -0.0307, -0.0442,\n         0.0008,  0.0808, -0.0988,  0.0611,  0.0153, -0.1655, -0.0123, -0.1634,\n        -0.0931, -0.0850, -0.0617,  0.0734, -0.1082, -0.1164, -0.0721, -0.0496,\n        -0.0486, -0.0532, -0.1327,  0.1764,  0.1442, -0.1105, -0.0012, -0.1561,\n         0.0330,  0.0404,  0.1469,  0.0566,  0.0840, -0.0878,  0.1157, -0.1351,\n         0.0693,  0.1190, -0.1562, -0.1200,  0.0829,  0.0111,  0.1334, -0.1629,\n         0.0170, -0.1678, -0.0959,  0.1689,  0.0483,  0.1137, -0.1555,  0.1747,\n        -0.0866,  0.0886, -0.1455,  0.1355,  0.1346, -0.0079, -0.1867,  0.0602,\n        -0.0795,  0.0720,  0.0013,  0.0657,  0.1314, -0.0996, -0.0352,  0.1578,\n        -0.1284, -0.1796,  0.1600,  0.1112, -0.0823, -0.1841, -0.1046,  0.0559]), 'meta_learner.transductive_net.feature_extractors.1.0.weight': tensor([[-0.0560, -0.0525,  0.0429,  ..., -0.1406, -0.0453, -0.0092],\n        [-0.1835, -0.1227, -0.0502,  ...,  0.1180,  0.1716, -0.0048],\n        [-0.0084, -0.1110,  0.1547,  ..., -0.0348, -0.1408, -0.0654],\n        ...,\n        [-0.1730, -0.0787,  0.1375,  ...,  0.1751,  0.1301, -0.0219],\n        [ 0.1412,  0.1039,  0.0795,  ..., -0.1349,  0.1209,  0.0811],\n        [ 0.0389,  0.0317,  0.0790,  ..., -0.0605, -0.0971, -0.0460]]), 'meta_learner.transductive_net.feature_extractors.1.0.bias': tensor([-0.0202, -0.1297, -0.0601,  0.1392, -0.0039, -0.0668,  0.1260,  0.0125,\n        -0.0473,  0.0094, -0.0787, -0.1658, -0.0227,  0.0840, -0.0997, -0.1133,\n         0.0504, -0.1832, -0.0342, -0.0007, -0.0702,  0.0322,  0.1651, -0.0748,\n         0.1394, -0.0735, -0.0780, -0.0141,  0.1746, -0.0878,  0.0283, -0.1781,\n         0.1001, -0.1486, -0.1114,  0.0684,  0.1086, -0.0871, -0.0639, -0.1342,\n        -0.0539, -0.0260, -0.1390,  0.0375, -0.0776, -0.1238,  0.1278, -0.1586,\n         0.0465, -0.1683,  0.1623,  0.0147,  0.0748, -0.0247, -0.0014, -0.0465,\n         0.0675, -0.0677,  0.0683,  0.0053,  0.0109,  0.0105,  0.1723,  0.1300]), 'meta_learner.transductive_net.feature_extractors.2.0.weight': tensor([[-0.0707,  0.0175, -0.0310,  ..., -0.1447,  0.0894, -0.0648],\n        [ 0.1679,  0.0826,  0.0854,  ..., -0.1420,  0.0770, -0.0311],\n        [ 0.1240,  0.1064, -0.1205,  ...,  0.0913, -0.1743, -0.1130],\n        ...,\n        [-0.0428,  0.0861,  0.1444,  ..., -0.1690,  0.0668, -0.0865],\n        [ 0.1748,  0.0011,  0.1110,  ..., -0.0390,  0.0933,  0.1632],\n        [ 0.0955,  0.0004,  0.0743,  ...,  0.1160, -0.1442, -0.0139]]), 'meta_learner.transductive_net.feature_extractors.2.0.bias': tensor([ 0.0646,  0.0474,  0.1679, -0.0165,  0.1126,  0.0812,  0.1653, -0.1214,\n        -0.1298, -0.0107,  0.0546, -0.0734, -0.1057,  0.1255, -0.0041,  0.0250,\n         0.0151, -0.1508,  0.1583, -0.1732, -0.0533,  0.1181,  0.0882,  0.0101,\n         0.0935,  0.1341,  0.0727, -0.0259, -0.1534,  0.1650, -0.0662,  0.1258,\n         0.0673,  0.0893, -0.1315,  0.0978, -0.1435,  0.0762,  0.0115, -0.1674,\n         0.1580, -0.1700,  0.0864,  0.1694,  0.1238, -0.0386, -0.0459, -0.0957,\n         0.0358, -0.1003,  0.0274, -0.0621,  0.1732,  0.1729, -0.1682, -0.0003,\n        -0.0600, -0.0747,  0.0734, -0.0685, -0.0992, -0.0222,  0.0588, -0.1021,\n        -0.0995,  0.0349, -0.0297, -0.0569, -0.0252,  0.0594, -0.1680, -0.0999,\n        -0.1052,  0.1034,  0.1011, -0.1357,  0.0764, -0.0235,  0.1156,  0.0745,\n        -0.0655, -0.1566, -0.1861, -0.0606,  0.1530, -0.1001,  0.0383,  0.1829,\n        -0.1170,  0.1119,  0.1461, -0.1179, -0.0873, -0.0097, -0.0320,  0.0267,\n         0.1516,  0.0463,  0.1235, -0.0949, -0.0089,  0.0183,  0.0743,  0.0094,\n         0.0079, -0.1018, -0.0712, -0.0872,  0.0864,  0.0209, -0.0221, -0.0129,\n         0.0644,  0.0190, -0.0205, -0.0239, -0.0552, -0.0430,  0.0059,  0.0303,\n         0.0302,  0.0896,  0.0096,  0.0835,  0.0258, -0.1235,  0.0899,  0.0173,\n        -0.1712,  0.1080,  0.1123, -0.1407, -0.1304, -0.0535,  0.0406,  0.0680,\n        -0.0274, -0.1313,  0.0631, -0.0501, -0.0226,  0.0336, -0.1713, -0.0009,\n        -0.0815,  0.0267,  0.0588, -0.0637,  0.0760, -0.0690, -0.0474,  0.1070,\n        -0.0207, -0.0570, -0.0443,  0.0976,  0.0376, -0.0963,  0.1308,  0.1078,\n         0.0468, -0.1440, -0.0614, -0.0174,  0.0271, -0.0863, -0.1405, -0.0111,\n         0.0805, -0.0352, -0.0036,  0.0408, -0.0409,  0.0850, -0.0618,  0.1270,\n        -0.0346,  0.1375,  0.0947, -0.1330,  0.0002,  0.0363, -0.1693,  0.0113,\n        -0.1089,  0.0748, -0.0321,  0.0969,  0.0170, -0.1466, -0.0517, -0.0335,\n         0.0756,  0.1036, -0.1748,  0.1052, -0.1543,  0.0486,  0.0956,  0.0843,\n         0.0805,  0.0580, -0.1188,  0.0103, -0.0262,  0.0528,  0.1388,  0.1455,\n        -0.1640,  0.0103, -0.1516, -0.1482,  0.0387, -0.0230,  0.0706,  0.1608,\n         0.1484,  0.1449, -0.0614,  0.0093, -0.0844,  0.1567, -0.0746,  0.0801,\n        -0.0889,  0.0786,  0.0733, -0.1242,  0.0992, -0.0637,  0.0861, -0.1830,\n         0.1652,  0.0387, -0.0707, -0.1413, -0.0233,  0.0730,  0.1737, -0.0799,\n         0.1819,  0.0542,  0.0747, -0.0929, -0.0546, -0.1503,  0.0469,  0.0465,\n        -0.0733,  0.1372,  0.0888,  0.1594, -0.1269,  0.0443, -0.1524, -0.1104]), 'meta_learner.transductive_net.feature_projection.0.weight': tensor([[-0.0249, -0.0269, -0.0407,  ..., -0.0047, -0.0322,  0.0212],\n        [-0.0215,  0.0370,  0.0450,  ...,  0.0074,  0.0367, -0.0428],\n        [-0.0307,  0.0390,  0.0431,  ...,  0.0433,  0.0452, -0.0080],\n        ...,\n        [-0.0203, -0.0032,  0.0486,  ...,  0.0063, -0.0194,  0.0419],\n        [-0.0348, -0.0135,  0.0090,  ...,  0.0428, -0.0172, -0.0002],\n        [-0.0492, -0.0182,  0.0586,  ...,  0.0107,  0.0331,  0.0400]]), 'meta_learner.transductive_net.feature_projection.0.bias': tensor([-0.0061, -0.0164, -0.0174, -0.0398, -0.0189,  0.0035,  0.0218,  0.0428,\n        -0.0301,  0.0358, -0.0261,  0.0272, -0.0404,  0.0070,  0.0411,  0.0337,\n        -0.0396, -0.0065, -0.0118,  0.0183, -0.0145,  0.0295,  0.0059, -0.0068,\n         0.0184,  0.0125,  0.0268,  0.0022,  0.0036,  0.0224,  0.0366, -0.0465,\n         0.0033, -0.0424,  0.0175,  0.0179, -0.0138,  0.0451,  0.0004,  0.0281,\n        -0.0377, -0.0205,  0.0467, -0.0502, -0.0107, -0.0312,  0.0066,  0.0171,\n        -0.0009, -0.0096,  0.0256,  0.0030, -0.0084, -0.0157, -0.0141,  0.0322,\n         0.0213,  0.0595, -0.0387,  0.0032,  0.0450,  0.0210,  0.0025,  0.0121]), 'meta_learner.transductive_net.adaptive_net.0.weight': tensor([[ 0.1193,  0.0529, -0.0517,  ..., -0.0253,  0.1239,  0.0198],\n        [ 0.0265,  0.0626,  0.1006,  ...,  0.0291,  0.0271, -0.0700],\n        [-0.0209, -0.0927,  0.0299,  ..., -0.1143,  0.0925,  0.0052],\n        ...,\n        [ 0.0357,  0.0575, -0.0909,  ...,  0.1232,  0.0481,  0.0861],\n        [-0.0811,  0.0515, -0.0042,  ..., -0.0685,  0.0764,  0.0191],\n        [-0.1022,  0.0405, -0.0620,  ...,  0.0118, -0.0874, -0.1000]]), 'meta_learner.transductive_net.adaptive_net.0.bias': tensor([ 0.0257, -0.0376,  0.0998, -0.0965, -0.0031,  0.0878,  0.0009, -0.0171,\n         0.0412,  0.1012,  0.0374,  0.0310, -0.1149, -0.0162, -0.0301, -0.0591,\n        -0.0017, -0.0988, -0.0060, -0.0819,  0.0105,  0.0675, -0.0388,  0.1213,\n         0.1240,  0.1104,  0.1097,  0.0882, -0.0878,  0.0018,  0.0863,  0.0531,\n         0.0818, -0.0027,  0.0711, -0.0884,  0.0743, -0.0356,  0.0094, -0.0885,\n         0.1007, -0.1105,  0.0558,  0.0502, -0.0063, -0.0536,  0.0053, -0.0090,\n         0.0697,  0.0718, -0.0794,  0.1215, -0.0132,  0.0330,  0.1205,  0.1167,\n         0.0538, -0.0806, -0.0976,  0.0452,  0.0914, -0.0630,  0.0727, -0.1167]), 'meta_learner.transductive_net.adaptive_net.3.weight': tensor([[ 0.0267, -0.0095, -0.0381,  ..., -0.0233, -0.0358,  0.0518],\n        [-0.0926, -0.1118,  0.0376,  ..., -0.0237, -0.0399, -0.0786],\n        [ 0.0115,  0.1149,  0.0311,  ..., -0.0566, -0.1173,  0.0006],\n        ...,\n        [ 0.0392,  0.0522,  0.0266,  ...,  0.0399, -0.1193, -0.0253],\n        [-0.0526, -0.0565,  0.1239,  ...,  0.0376, -0.0866, -0.0481],\n        [-0.0638, -0.0378, -0.0068,  ..., -0.0936, -0.0683,  0.0016]]), 'meta_learner.transductive_net.adaptive_net.3.bias': tensor([ 0.0455,  0.0498,  0.0815,  0.0566,  0.0293,  0.0274, -0.0471, -0.0018,\n        -0.0409,  0.1197, -0.0750, -0.0763,  0.0136, -0.0129,  0.0783,  0.1193,\n        -0.1174,  0.0922,  0.0346,  0.0998, -0.1063,  0.0552, -0.1232,  0.0120,\n         0.0281,  0.0188, -0.1219, -0.0805, -0.0008,  0.0542, -0.0874,  0.0765]), 'meta_learner.transductive_net.adaptive_net.6.weight': tensor([[-0.0385,  0.0673,  0.1108,  0.0979,  0.1496, -0.0716, -0.0968,  0.1446,\n          0.0860,  0.1473, -0.1305, -0.1032,  0.1742,  0.0291,  0.0070,  0.1329,\n         -0.0405,  0.0421, -0.1247, -0.1079,  0.1128,  0.1089,  0.0694, -0.0664,\n         -0.1346, -0.0756,  0.0618, -0.1395,  0.1121,  0.0712, -0.0533, -0.1398],\n        [-0.0184, -0.0519,  0.0560, -0.1062, -0.0953, -0.1454, -0.0826, -0.1568,\n         -0.0240, -0.1253, -0.0668, -0.0187,  0.0552, -0.0243,  0.1226,  0.1348,\n         -0.1254, -0.0788, -0.0334,  0.1395, -0.1063,  0.1677,  0.0629,  0.1035,\n          0.1455,  0.0221, -0.0491, -0.0642, -0.0795,  0.0691,  0.0766,  0.0586]]), 'meta_learner.transductive_net.adaptive_net.6.bias': tensor([ 0.1162, -0.0662]), 'meta_learner.transductive_net.attention_net.in_proj_weight': tensor([[ 0.1499, -0.1297, -0.0400,  ...,  0.0069,  0.1297, -0.0316],\n        [ 0.0651,  0.1326, -0.1040,  ..., -0.1450,  0.0255, -0.0074],\n        [ 0.0912, -0.0046,  0.1084,  ...,  0.0344,  0.0308,  0.0787],\n        ...,\n        [ 0.0304,  0.0523, -0.0867,  ...,  0.0757, -0.0210,  0.1175],\n        [-0.0734,  0.1009, -0.0293,  ..., -0.1144,  0.0155, -0.0095],\n        [ 0.1256, -0.0447,  0.0847,  ...,  0.1312, -0.0769, -0.1114]]), 'meta_learner.transductive_net.attention_net.in_proj_bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'meta_learner.transductive_net.attention_net.out_proj.weight': tensor([[ 0.0873, -0.0201, -0.0694,  ...,  0.0288, -0.0274,  0.0071],\n        [ 0.0220, -0.0683, -0.0115,  ...,  0.0162,  0.0030,  0.0250],\n        [ 0.0620, -0.0258, -0.0088,  ..., -0.0979,  0.1243,  0.0966],\n        ...,\n        [-0.0169,  0.1037, -0.0049,  ..., -0.0238, -0.0969, -0.0654],\n        [ 0.0961, -0.0908,  0.0204,  ...,  0.0248,  0.1000,  0.0646],\n        [-0.1153, -0.0332, -0.0012,  ..., -0.0973, -0.1118, -0.0163]]), 'meta_learner.transductive_net.attention_net.out_proj.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'meta_learner.transductive_net.self_attention.in_proj_weight': tensor([[ 0.0196,  0.1377, -0.1373,  ...,  0.1154, -0.0331, -0.0211],\n        [-0.1411, -0.0471,  0.0186,  ...,  0.0103, -0.1408,  0.0334],\n        [ 0.0730,  0.0805,  0.0386,  ..., -0.0396,  0.0846,  0.1527],\n        ...,\n        [-0.0759, -0.1407, -0.0347,  ...,  0.1475, -0.0775, -0.0902],\n        [-0.1009,  0.0564,  0.0536,  ...,  0.0929, -0.0519, -0.1573],\n        [-0.0754, -0.0278, -0.0107,  ..., -0.0311,  0.0123,  0.0217]]), 'meta_learner.transductive_net.self_attention.in_proj_bias': tensor([-5.0038e-03, -3.0762e-03,  8.0875e-03, -7.7520e-03,  2.1510e-03,\n         1.9800e-02,  7.8164e-03,  2.3775e-02, -5.2583e-03,  4.9721e-03,\n        -3.8946e-03, -3.6472e-03,  8.8065e-03, -7.8682e-03, -9.9006e-04,\n        -6.4832e-03,  1.1766e-02,  1.1669e-03,  8.5576e-03, -4.2337e-03,\n        -1.6038e-02,  2.6319e-03,  1.8207e-03, -3.8499e-03,  1.6547e-02,\n        -2.1596e-02, -3.0693e-02,  1.0810e-03, -4.0453e-03, -7.8605e-03,\n        -1.0465e-03, -1.9895e-02, -8.2152e-03, -3.0281e-03, -1.3732e-02,\n         3.8523e-04, -5.1909e-03, -2.1059e-02,  1.3534e-02, -7.0686e-03,\n        -6.3811e-03, -5.7634e-03,  3.5225e-03,  5.0422e-03, -5.0580e-03,\n        -9.5585e-03, -9.1316e-03, -8.2519e-03, -1.5377e-03, -5.4518e-03,\n        -1.8740e-03, -5.0656e-03, -1.1001e-02, -1.2481e-02,  2.6061e-05,\n         1.7972e-02, -3.7834e-03, -5.2557e-03, -7.4792e-03, -6.1618e-03,\n         1.3951e-02,  9.2639e-03,  1.1167e-02,  1.5234e-03,  2.4126e-05,\n         1.7729e-05, -1.6655e-05, -1.5519e-04, -3.2619e-05, -9.6238e-05,\n        -8.9526e-05,  6.4631e-05,  5.7579e-05,  1.6859e-06,  5.0499e-05,\n         1.7604e-05,  6.6092e-06, -1.6884e-04, -8.2855e-05,  8.7321e-05,\n         8.1473e-06,  1.0980e-04, -2.7867e-05, -2.0206e-05,  1.3275e-05,\n         4.2025e-05, -3.7398e-06,  4.8007e-05, -1.6861e-04,  6.5536e-05,\n        -4.5742e-05,  1.3039e-04,  2.2033e-05, -9.0191e-06, -3.6150e-05,\n        -2.0407e-05, -3.7570e-05,  3.3456e-05,  1.2988e-04, -1.5135e-04,\n        -2.0391e-05,  1.2765e-04,  6.6724e-05,  1.8228e-04, -6.4270e-05,\n        -1.3218e-04, -3.9585e-05,  1.6701e-04, -6.5531e-05,  1.3794e-04,\n         1.6448e-04,  3.2394e-06, -5.1730e-05,  3.5905e-06,  7.4256e-05,\n         5.4891e-05, -5.5116e-05, -7.1186e-05,  1.4684e-04,  1.2932e-05,\n         3.2359e-05, -3.7418e-05,  7.4564e-05, -1.7231e-05, -1.9396e-04,\n        -2.6990e-05, -2.4320e-05, -3.8021e-05, -2.9144e-04,  1.1918e-03,\n        -2.2057e-05, -1.7136e-03, -8.0534e-04,  9.3760e-05,  1.2776e-03,\n         3.3803e-05,  6.0918e-04,  6.1869e-04,  1.4101e-03,  9.6595e-04,\n         5.0059e-04, -2.5595e-05,  7.8370e-05, -7.9636e-04,  3.0018e-04,\n         7.2033e-04,  1.0380e-04,  5.7396e-04, -8.8690e-04,  2.2713e-04,\n         6.8444e-04,  6.1990e-04, -7.1307e-04,  6.6014e-04, -2.3053e-03,\n        -9.7212e-04, -6.2676e-04,  5.8119e-04,  1.3291e-03,  9.7847e-04,\n        -8.4900e-04,  7.7347e-04, -1.2082e-03, -1.5030e-03,  9.1076e-04,\n        -9.8735e-05,  6.7782e-04, -3.4874e-04,  4.1845e-04,  1.1230e-04,\n         5.8156e-04, -2.8307e-04, -9.7978e-04, -9.2771e-04,  6.3617e-04,\n        -3.8065e-04, -3.0688e-04, -1.4355e-03, -1.3753e-03, -1.4605e-04,\n         1.2636e-04,  3.8062e-06, -7.1660e-04, -9.8839e-04, -1.2073e-03,\n         1.5198e-03, -9.8689e-04, -3.5492e-04,  6.0460e-04,  1.0467e-03,\n         1.2476e-03,  1.0146e-03]), 'meta_learner.transductive_net.self_attention.out_proj.weight': tensor([[ 0.0469, -0.0009, -0.0812,  ...,  0.0474,  0.1290, -0.1011],\n        [ 0.0969,  0.0131, -0.0863,  ...,  0.0717,  0.1255, -0.0374],\n        [-0.0983,  0.0646,  0.0942,  ..., -0.0728,  0.0461,  0.0370],\n        ...,\n        [-0.0002, -0.0962,  0.0759,  ..., -0.0479,  0.0420, -0.0591],\n        [-0.0732,  0.0362, -0.1381,  ..., -0.0526, -0.0820, -0.1065],\n        [ 0.0952, -0.0738, -0.0467,  ...,  0.0114, -0.0572,  0.0255]]), 'meta_learner.transductive_net.self_attention.out_proj.bias': tensor([-3.5787e-04,  5.1847e-04,  2.6779e-04,  4.8446e-04,  3.2970e-04,\n        -1.4809e-03,  4.0465e-04, -1.7711e-04,  4.5920e-04,  2.7125e-04,\n        -1.2235e-03, -3.2926e-04,  7.7761e-06, -6.3550e-04,  6.8773e-04,\n        -1.1456e-03, -9.8860e-04, -7.1686e-04, -6.3966e-05,  1.3135e-03,\n         1.0053e-03, -2.8868e-05, -1.9025e-04,  6.7043e-04, -4.6977e-04,\n        -1.8566e-03, -1.3768e-03, -3.8069e-04, -3.6570e-04,  6.0074e-04,\n         4.2293e-05,  4.3712e-05, -2.0196e-04,  4.7375e-05, -1.9815e-04,\n         4.7464e-04, -6.8589e-04,  1.0201e-04,  8.3182e-04, -2.8519e-04,\n         1.7666e-04, -1.8020e-04, -5.2952e-05, -4.6505e-05,  7.5994e-04,\n        -1.0699e-04, -2.3258e-04, -3.6451e-04, -1.0478e-03, -1.3357e-04,\n         1.3818e-04, -7.5787e-04, -1.6463e-03,  1.9294e-03, -6.6718e-04,\n        -2.1192e-04,  9.0547e-04,  9.3684e-05,  1.9477e-04, -9.1946e-04,\n        -4.1333e-04,  5.3695e-04,  9.4458e-05,  3.4894e-04]), 'meta_learner.transductive_net.graph_conv.weight': tensor([[ 0.0630, -0.0885, -0.0087,  ...,  0.0595,  0.0228, -0.0685],\n        [ 0.0993, -0.1186, -0.0224,  ..., -0.0206,  0.0375, -0.1219],\n        [ 0.0575, -0.0640, -0.0260,  ...,  0.0789, -0.0564,  0.0113],\n        ...,\n        [ 0.0439, -0.0565, -0.0302,  ...,  0.1215,  0.1201,  0.1122],\n        [ 0.0219, -0.1014,  0.1182,  ..., -0.0222,  0.1147, -0.0791],\n        [-0.1208, -0.1128, -0.0077,  ..., -0.0572,  0.0240,  0.0366]]), 'meta_learner.transductive_net.graph_conv.bias': tensor([ 0.1129,  0.0705, -0.0254, -0.0198, -0.0667,  0.0024, -0.0163, -0.0188,\n        -0.0896, -0.0651,  0.1055, -0.0273, -0.0246,  0.0073, -0.0513,  0.0978,\n        -0.0067,  0.0496,  0.0247,  0.1243,  0.1149,  0.0299,  0.1190, -0.0572,\n        -0.0419,  0.0216, -0.0767,  0.0992,  0.0492, -0.1185,  0.0614, -0.0809,\n         0.0374,  0.0456,  0.0666, -0.0450, -0.0677,  0.0561, -0.0678,  0.0621,\n        -0.0598,  0.1221, -0.0775,  0.0065,  0.1133, -0.0553, -0.0043,  0.1035,\n        -0.1206, -0.0777, -0.0676,  0.0347,  0.0844,  0.1223,  0.1247, -0.0056,\n        -0.0112, -0.0825, -0.1192, -0.0576, -0.0924,  0.1123,  0.0925, -0.0151]), 'meta_learner.transductive_net.layer_norm.weight': tensor([0.9998, 1.0058, 0.9965, 0.9898, 0.9888, 1.0055, 0.9796, 1.0027, 0.9999,\n        0.9862, 0.9937, 0.9984, 1.0145, 0.9855, 0.9892, 0.9909, 0.9727, 1.0072,\n        1.0087, 0.9950, 1.0222, 0.9900, 1.0069, 1.0018, 0.9902, 0.9976, 0.9970,\n        1.0049, 0.9921, 0.9849, 1.0108, 0.9736, 0.9951, 1.0098, 0.9955, 1.0070,\n        1.0085, 1.0070, 0.9857, 0.9925, 0.9938, 0.9967, 0.9853, 0.9843, 1.0051,\n        0.9994, 0.9962, 0.9878, 0.9852, 1.0098, 0.9941, 0.9867, 0.9962, 1.0112,\n        0.9982, 0.9972, 0.9974, 1.0218, 1.0065, 1.0069, 0.9903, 1.0165, 0.9831,\n        1.0170]), 'meta_learner.transductive_net.layer_norm.bias': tensor([-0.0012,  0.0030,  0.0167, -0.0008, -0.0090, -0.0312, -0.0061, -0.0022,\n        -0.0187, -0.0082,  0.0030,  0.0226, -0.0126, -0.0049,  0.0182,  0.0106,\n        -0.0136, -0.0007, -0.0067, -0.0021, -0.0039,  0.0144,  0.0045,  0.0008,\n        -0.0003, -0.0102,  0.0051,  0.0111, -0.0004, -0.0151,  0.0128,  0.0019,\n        -0.0060,  0.0088,  0.0031,  0.0048, -0.0040, -0.0033,  0.0075, -0.0066,\n        -0.0068,  0.0082, -0.0033,  0.0037,  0.0115,  0.0169, -0.0056, -0.0113,\n        -0.0129,  0.0212, -0.0120,  0.0123, -0.0138,  0.0065,  0.0141,  0.0148,\n         0.0020, -0.0061, -0.0044, -0.0079, -0.0099, -0.0098,  0.0119, -0.0033])}, sample_count=23275, training_loss=np.float64(0.07468851774723993), validation_accuracy=np.float64(0.8866666820314195), validation_precision=0.0, validation_recall=0.0, validation_f1_score=0.0, timestamp=1760153437.5194602, model_hash='b662a5255fe473df8ac7fa831ad63b58cd2feb3420f6358446b8c602421a6c10', ipfs_cid=None, blockchain_tx_hash=None)",
        "ClientUpdate(client_id='client_2', model_parameters={'meta_learner.transductive_net.feature_extractors.0.0.weight': tensor([[-0.1698, -0.0900, -0.0947,  ...,  0.1538, -0.0004, -0.1809],\n        [ 0.0999,  0.0824, -0.0905,  ...,  0.0023, -0.1471,  0.0284],\n        [ 0.0264, -0.0425,  0.0474,  ...,  0.1160, -0.0065, -0.0991],\n        ...,\n        [ 0.1130, -0.0392, -0.1248,  ...,  0.1434,  0.1059,  0.1561],\n        [ 0.0039,  0.1394, -0.1680,  ...,  0.0866, -0.0618,  0.1428],\n        [-0.0507,  0.0735,  0.0812,  ...,  0.1095, -0.0277, -0.0013]]), 'meta_learner.transductive_net.feature_extractors.0.0.bias': tensor([ 0.1437, -0.0378, -0.1498,  0.1314, -0.0499,  0.1191,  0.1473,  0.0688,\n        -0.0457, -0.0656,  0.1312,  0.0709, -0.0212, -0.0795, -0.1257, -0.0963,\n         0.0103,  0.0566, -0.0101,  0.1778,  0.0738,  0.0205,  0.0722, -0.0076,\n         0.0492,  0.0811,  0.0502,  0.0929,  0.0642,  0.0039, -0.0155,  0.1300,\n         0.1087,  0.0036, -0.0360,  0.1116, -0.0283, -0.1695, -0.0799,  0.1329,\n        -0.0309, -0.0075,  0.1627, -0.0200, -0.1211, -0.1244, -0.1175,  0.1453,\n        -0.1210,  0.0146, -0.1062, -0.0880,  0.0345,  0.1522, -0.0304, -0.0428,\n        -0.0116,  0.0613, -0.1142,  0.0441,  0.0057, -0.1559, -0.0160, -0.1600,\n        -0.0872, -0.1054, -0.0632,  0.0755, -0.0895, -0.1357, -0.0896, -0.0403,\n        -0.0483, -0.0536, -0.1229,  0.1808,  0.1347, -0.1161, -0.0136, -0.1615,\n         0.0347,  0.0258,  0.1336,  0.0781,  0.0860, -0.0986,  0.1168, -0.1278,\n         0.0691,  0.1085, -0.1555, -0.1304,  0.0722,  0.0122,  0.1247, -0.1260,\n         0.0345, -0.1703, -0.0896,  0.1708,  0.0453,  0.1012, -0.1426,  0.1653,\n        -0.0960,  0.0794, -0.1412,  0.1429,  0.1306,  0.0007, -0.1824,  0.0579,\n        -0.0900,  0.0635,  0.0012,  0.0823,  0.1190, -0.0970, -0.0340,  0.1891,\n        -0.1417, -0.1678,  0.1541,  0.0902, -0.0734, -0.1600, -0.1017,  0.0427]), 'meta_learner.transductive_net.feature_extractors.1.0.weight': tensor([[-0.0494, -0.0603,  0.0356,  ..., -0.1478, -0.0453,  0.0028],\n        [-0.1840, -0.1168, -0.0439,  ...,  0.1217,  0.1716, -0.0063],\n        [-0.0134, -0.1055,  0.1530,  ..., -0.0151, -0.1408, -0.0675],\n        ...,\n        [-0.1604, -0.0844,  0.1391,  ...,  0.1872,  0.1301, -0.0099],\n        [ 0.1317,  0.1126,  0.0934,  ..., -0.1562,  0.1209,  0.0696],\n        [ 0.0505,  0.0064,  0.0499,  ..., -0.0497, -0.0971, -0.0438]]), 'meta_learner.transductive_net.feature_extractors.1.0.bias': tensor([-0.0117, -0.1362, -0.0606,  0.1420, -0.0071, -0.0742,  0.1371,  0.0036,\n        -0.0398,  0.0022, -0.0849, -0.1595, -0.0281,  0.0761, -0.1132, -0.0905,\n         0.0380, -0.2005, -0.0457,  0.0112, -0.0703,  0.0219,  0.1652, -0.0788,\n         0.1485, -0.0629, -0.0651, -0.0087,  0.1654, -0.0976,  0.0166, -0.1847,\n         0.1037, -0.1395, -0.1021,  0.0653,  0.1073, -0.0931, -0.0684, -0.1421,\n        -0.0698, -0.0309, -0.1318,  0.0310, -0.0839, -0.1065,  0.1251, -0.1441,\n         0.0445, -0.1601,  0.1383,  0.0425,  0.0562, -0.0419, -0.0179, -0.0524,\n         0.0710, -0.0655,  0.0636,  0.0074, -0.0041, -0.0060,  0.1665,  0.1429]), 'meta_learner.transductive_net.feature_extractors.2.0.weight': tensor([[-0.0882,  0.0032, -0.0443,  ..., -0.1431,  0.0894, -0.0770],\n        [ 0.1873,  0.0681,  0.0773,  ..., -0.1231,  0.0770, -0.0239],\n        [ 0.1546,  0.0790, -0.1534,  ...,  0.1131, -0.1743, -0.0804],\n        ...,\n        [-0.0373,  0.0928,  0.1426,  ..., -0.1727,  0.0668, -0.0802],\n        [ 0.1582,  0.0177,  0.1333,  ..., -0.0633,  0.0933,  0.1500],\n        [ 0.1000, -0.0031,  0.0614,  ...,  0.1182, -0.1442,  0.0035]]), 'meta_learner.transductive_net.feature_extractors.2.0.bias': tensor([ 0.0630,  0.0671,  0.1782,  0.0076,  0.1134,  0.0973,  0.1529, -0.1251,\n        -0.1281, -0.0094,  0.0483, -0.0791, -0.1212,  0.1428,  0.0073,  0.0373,\n         0.0264, -0.1350,  0.1436, -0.1726, -0.0454,  0.1341,  0.1105,  0.0142,\n         0.0998,  0.1167,  0.0569, -0.0310, -0.1550,  0.1659, -0.0660,  0.1430,\n         0.0553,  0.0807, -0.1159,  0.1024, -0.1383,  0.0652,  0.0209, -0.1496,\n         0.1550, -0.1629,  0.0845,  0.1576,  0.1276, -0.0432, -0.0457, -0.0918,\n         0.0540, -0.0887,  0.0277, -0.0709,  0.1508,  0.1614, -0.1638,  0.0089,\n        -0.0655, -0.0817,  0.0676, -0.0535, -0.1038, -0.0255,  0.0513, -0.1008,\n        -0.1032,  0.0383, -0.0196, -0.0592, -0.0224,  0.0410, -0.1500, -0.1178,\n        -0.1183,  0.1220,  0.1000, -0.1374,  0.0737, -0.0249,  0.1013,  0.0690,\n        -0.0732, -0.1703, -0.1800, -0.0498,  0.1532, -0.0865,  0.0528,  0.1628,\n        -0.1235,  0.0771,  0.1511, -0.1490, -0.0848, -0.0271, -0.0364,  0.0246,\n         0.1344,  0.0318,  0.1339, -0.1103, -0.0054,  0.0175,  0.0825,  0.0176,\n         0.0051, -0.1081, -0.0632, -0.0761,  0.0846,  0.0196, -0.0490, -0.0335,\n         0.0546,  0.0420, -0.0121, -0.0344, -0.0362, -0.0402,  0.0003,  0.0091,\n         0.0080,  0.0741,  0.0028,  0.0878,  0.0101, -0.1060,  0.0847,  0.0237,\n        -0.1712,  0.0901,  0.1110, -0.1436, -0.1204, -0.0516,  0.0241,  0.0583,\n        -0.0334, -0.1444,  0.0502, -0.0498, -0.0253,  0.0343, -0.1725, -0.0139,\n        -0.1076,  0.0472,  0.0489, -0.0488,  0.0901, -0.0657, -0.0388,  0.0971,\n        -0.0385, -0.0422, -0.0477,  0.0991,  0.0391, -0.0856,  0.1217,  0.1072,\n         0.0648, -0.1392, -0.0608,  0.0011,  0.0161, -0.0931, -0.1472, -0.0314,\n         0.0809, -0.0222,  0.0128,  0.0354, -0.0430,  0.1051, -0.0657,  0.1366,\n        -0.0277,  0.1361,  0.0855, -0.1257,  0.0002,  0.0138, -0.1839,  0.0427,\n        -0.1017,  0.0587, -0.0106,  0.0816,  0.0260, -0.1328, -0.0407, -0.0397,\n         0.0785,  0.1154, -0.1706,  0.0996, -0.1485,  0.0484,  0.0892,  0.1057,\n         0.0793,  0.0587, -0.1110, -0.0043, -0.0188,  0.0408,  0.1394,  0.1633,\n        -0.1660,  0.0132, -0.1374, -0.1638,  0.0566, -0.0281,  0.0694,  0.1812,\n         0.1245,  0.1475, -0.0601,  0.0052, -0.1144,  0.1548, -0.0676,  0.0720,\n        -0.0889,  0.0524,  0.0757, -0.1210,  0.0970, -0.0683,  0.0802, -0.1612,\n         0.1561,  0.0295, -0.0635, -0.1468, -0.0122,  0.0426,  0.1775, -0.0943,\n         0.1806,  0.0614,  0.0846, -0.0720, -0.0564, -0.1403,  0.0333,  0.0195,\n        -0.0848,  0.1494,  0.0931,  0.1550, -0.1133,  0.0425, -0.1756, -0.1160]), 'meta_learner.transductive_net.feature_projection.0.weight': tensor([[-0.0183, -0.0334, -0.0243,  ...,  0.0005, -0.0337,  0.0303],\n        [-0.0439,  0.0127,  0.0568,  ..., -0.0023,  0.0242, -0.0394],\n        [-0.0344,  0.0231,  0.0441,  ...,  0.0380,  0.0305, -0.0116],\n        ...,\n        [-0.0207, -0.0065,  0.0246,  ...,  0.0169, -0.0116,  0.0294],\n        [-0.0289, -0.0178,  0.0158,  ...,  0.0117, -0.0104, -0.0053],\n        [-0.0387,  0.0046,  0.0214,  ...,  0.0045,  0.0281,  0.0362]]), 'meta_learner.transductive_net.feature_projection.0.bias': tensor([-0.0092, -0.0216, -0.0353, -0.0412, -0.0203, -0.0139,  0.0023,  0.0295,\n        -0.0345,  0.0269, -0.0237,  0.0121, -0.0275,  0.0150,  0.0303,  0.0309,\n        -0.0531,  0.0041, -0.0165,  0.0049, -0.0233,  0.0283,  0.0096, -0.0163,\n         0.0032,  0.0163,  0.0296, -0.0179, -0.0042,  0.0240,  0.0393, -0.0432,\n         0.0161, -0.0483,  0.0216, -0.0015, -0.0016,  0.0225,  0.0102,  0.0312,\n        -0.0192, -0.0008,  0.0494, -0.0303, -0.0151, -0.0196,  0.0104,  0.0181,\n        -0.0101, -0.0221,  0.0357,  0.0133, -0.0067, -0.0175, -0.0016,  0.0163,\n         0.0124,  0.0488, -0.0371,  0.0034,  0.0479,  0.0117, -0.0028,  0.0038]), 'meta_learner.transductive_net.adaptive_net.0.weight': tensor([[ 0.1193,  0.0529, -0.0517,  ..., -0.0253,  0.1239,  0.0198],\n        [ 0.0265,  0.0626,  0.1006,  ...,  0.0291,  0.0271, -0.0700],\n        [-0.0209, -0.0927,  0.0299,  ..., -0.1143,  0.0925,  0.0052],\n        ...,\n        [ 0.0357,  0.0575, -0.0909,  ...,  0.1232,  0.0481,  0.0861],\n        [-0.0811,  0.0515, -0.0042,  ..., -0.0685,  0.0764,  0.0191],\n        [-0.1022,  0.0405, -0.0620,  ...,  0.0118, -0.0874, -0.1000]]), 'meta_learner.transductive_net.adaptive_net.0.bias': tensor([ 0.0257, -0.0376,  0.0998, -0.0965, -0.0031,  0.0878,  0.0009, -0.0171,\n         0.0412,  0.1012,  0.0374,  0.0310, -0.1149, -0.0162, -0.0301, -0.0591,\n        -0.0017, -0.0988, -0.0060, -0.0819,  0.0105,  0.0675, -0.0388,  0.1213,\n         0.1240,  0.1104,  0.1097,  0.0882, -0.0878,  0.0018,  0.0863,  0.0531,\n         0.0818, -0.0027,  0.0711, -0.0884,  0.0743, -0.0356,  0.0094, -0.0885,\n         0.1007, -0.1105,  0.0558,  0.0502, -0.0063, -0.0536,  0.0053, -0.0090,\n         0.0697,  0.0718, -0.0794,  0.1215, -0.0132,  0.0330,  0.1205,  0.1167,\n         0.0538, -0.0806, -0.0976,  0.0452,  0.0914, -0.0630,  0.0727, -0.1167]), 'meta_learner.transductive_net.adaptive_net.3.weight': tensor([[ 0.0267, -0.0095, -0.0381,  ..., -0.0233, -0.0358,  0.0518],\n        [-0.0926, -0.1118,  0.0376,  ..., -0.0237, -0.0399, -0.0786],\n        [ 0.0115,  0.1149,  0.0311,  ..., -0.0566, -0.1173,  0.0006],\n        ...,\n        [ 0.0392,  0.0522,  0.0266,  ...,  0.0399, -0.1193, -0.0253],\n        [-0.0526, -0.0565,  0.1239,  ...,  0.0376, -0.0866, -0.0481],\n        [-0.0638, -0.0378, -0.0068,  ..., -0.0936, -0.0683,  0.0016]]), 'meta_learner.transductive_net.adaptive_net.3.bias': tensor([ 0.0455,  0.0498,  0.0815,  0.0566,  0.0293,  0.0274, -0.0471, -0.0018,\n        -0.0409,  0.1197, -0.0750, -0.0763,  0.0136, -0.0129,  0.0783,  0.1193,\n        -0.1174,  0.0922,  0.0346,  0.0998, -0.1063,  0.0552, -0.1232,  0.0120,\n         0.0281,  0.0188, -0.1219, -0.0805, -0.0008,  0.0542, -0.0874,  0.0765]), 'meta_learner.transductive_net.adaptive_net.6.weight': tensor([[-0.0385,  0.0673,  0.1108,  0.0979,  0.1496, -0.0716, -0.0968,  0.1446,\n          0.0860,  0.1473, -0.1305, -0.1032,  0.1742,  0.0291,  0.0070,  0.1329,\n         -0.0405,  0.0421, -0.1247, -0.1079,  0.1128,  0.1089,  0.0694, -0.0664,\n         -0.1346, -0.0756,  0.0618, -0.1395,  0.1121,  0.0712, -0.0533, -0.1398],\n        [-0.0184, -0.0519,  0.0560, -0.1062, -0.0953, -0.1454, -0.0826, -0.1568,\n         -0.0240, -0.1253, -0.0668, -0.0187,  0.0552, -0.0243,  0.1226,  0.1348,\n         -0.1254, -0.0788, -0.0334,  0.1395, -0.1063,  0.1677,  0.0629,  0.1035,\n          0.1455,  0.0221, -0.0491, -0.0642, -0.0795,  0.0691,  0.0766,  0.0586]]), 'meta_learner.transductive_net.adaptive_net.6.bias': tensor([ 0.1162, -0.0662]), 'meta_learner.transductive_net.attention_net.in_proj_weight': tensor([[ 0.1499, -0.1297, -0.0400,  ...,  0.0069,  0.1297, -0.0316],\n        [ 0.0651,  0.1326, -0.1040,  ..., -0.1450,  0.0255, -0.0074],\n        [ 0.0912, -0.0046,  0.1084,  ...,  0.0344,  0.0308,  0.0787],\n        ...,\n        [ 0.0304,  0.0523, -0.0867,  ...,  0.0757, -0.0210,  0.1175],\n        [-0.0734,  0.1009, -0.0293,  ..., -0.1144,  0.0155, -0.0095],\n        [ 0.1256, -0.0447,  0.0847,  ...,  0.1312, -0.0769, -0.1114]]), 'meta_learner.transductive_net.attention_net.in_proj_bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'meta_learner.transductive_net.attention_net.out_proj.weight': tensor([[ 0.0873, -0.0201, -0.0694,  ...,  0.0288, -0.0274,  0.0071],\n        [ 0.0220, -0.0683, -0.0115,  ...,  0.0162,  0.0030,  0.0250],\n        [ 0.0620, -0.0258, -0.0088,  ..., -0.0979,  0.1243,  0.0966],\n        ...,\n        [-0.0169,  0.1037, -0.0049,  ..., -0.0238, -0.0969, -0.0654],\n        [ 0.0961, -0.0908,  0.0204,  ...,  0.0248,  0.1000,  0.0646],\n        [-0.1153, -0.0332, -0.0012,  ..., -0.0973, -0.1118, -0.0163]]), 'meta_learner.transductive_net.attention_net.out_proj.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'meta_learner.transductive_net.self_attention.in_proj_weight': tensor([[-0.0004,  0.1630, -0.1304,  ...,  0.0982, -0.0570, -0.0179],\n        [-0.1170, -0.0453, -0.0052,  ...,  0.0057, -0.1411,  0.0373],\n        [ 0.0793,  0.0943,  0.0420,  ..., -0.0459,  0.0818,  0.1431],\n        ...,\n        [-0.0500, -0.1329, -0.0363,  ...,  0.1317, -0.0719, -0.0728],\n        [-0.0977,  0.0681,  0.0670,  ...,  0.1278, -0.0700, -0.1310],\n        [-0.0401, -0.0155, -0.0070,  ..., -0.0002,  0.0066,  0.0342]]), 'meta_learner.transductive_net.self_attention.in_proj_bias': tensor([ 2.1018e-04, -2.4783e-03, -6.4237e-03,  4.0656e-03,  4.8321e-03,\n         1.2900e-02,  1.6005e-02, -9.7231e-04,  6.0527e-03,  1.2589e-02,\n         1.6680e-02,  1.6970e-02, -1.6381e-03,  3.1968e-03,  6.3135e-03,\n         9.9633e-03, -9.8098e-03,  1.9964e-02,  8.7793e-03,  3.1933e-03,\n        -9.3850e-04, -5.1968e-03,  8.6413e-03,  1.7070e-02,  1.4460e-02,\n        -1.7574e-02, -5.1709e-03, -3.7410e-03,  1.0264e-02, -1.2324e-02,\n         5.5218e-03,  1.0958e-02,  2.0712e-03,  8.1646e-03,  4.1018e-03,\n        -7.0770e-03, -6.8151e-03,  1.1662e-02,  5.1287e-04,  1.0420e-02,\n        -1.6504e-02,  3.9974e-03, -1.6295e-02,  3.7431e-03,  9.3092e-03,\n         7.3741e-03, -8.2839e-03, -1.7528e-02, -1.3629e-02,  5.7274e-03,\n        -1.3454e-02, -1.2523e-03,  9.5341e-03,  3.8608e-03, -9.1388e-03,\n        -3.6282e-03,  1.1730e-02,  4.4143e-03, -4.8647e-03,  1.2871e-02,\n         9.8632e-03,  2.1040e-03, -4.0077e-03, -1.1019e-02,  3.8939e-06,\n         1.5636e-05,  2.3262e-05,  2.2630e-05, -2.0453e-05,  7.5658e-05,\n         8.8032e-05,  4.3490e-05, -9.5201e-05, -1.7271e-05, -6.2647e-05,\n        -5.9772e-06,  8.4553e-05, -5.9628e-05,  5.0368e-05,  4.8735e-05,\n        -3.3208e-06, -5.1943e-05,  1.4256e-05, -5.2277e-05, -4.2917e-05,\n        -2.8177e-05, -2.1765e-05,  5.7722e-05, -9.7936e-05,  1.4896e-05,\n         2.0427e-05, -4.1476e-05, -7.2545e-05,  1.2110e-05, -8.1487e-05,\n         1.2811e-05, -7.8827e-06, -8.8025e-05,  3.3209e-05,  1.9213e-05,\n        -3.8954e-05,  4.9610e-05, -3.1889e-05, -2.4445e-06, -4.4377e-05,\n        -7.0204e-05,  1.9132e-05, -2.6714e-05,  4.0220e-05,  3.7824e-05,\n        -3.6079e-05,  9.1047e-05, -1.4935e-06,  2.3779e-06, -5.4853e-05,\n         5.6983e-05,  3.5156e-06, -6.9881e-05, -1.1743e-06, -2.9247e-05,\n         4.0518e-05,  6.7209e-05, -4.3411e-06, -9.4587e-05,  9.4419e-05,\n         3.1916e-05, -4.1336e-05,  3.1007e-05, -4.8030e-04,  6.0756e-04,\n         4.2521e-04, -1.4237e-03, -7.8450e-04,  3.4296e-04, -8.6246e-04,\n         7.4711e-04, -6.1012e-04,  1.2616e-03,  1.0908e-03,  1.3859e-03,\n         1.9487e-03, -2.1551e-03, -1.0568e-03, -1.3722e-03,  1.0901e-03,\n         4.7963e-04,  8.0486e-04,  1.3293e-04,  1.1080e-03,  8.1263e-04,\n        -1.6289e-03, -1.0031e-04,  1.7734e-03,  5.9649e-04,  1.6748e-03,\n        -1.7948e-04,  3.0323e-04,  7.8460e-04, -4.4237e-04,  1.3086e-04,\n        -3.1233e-03,  3.1444e-04, -8.6661e-04,  3.8964e-04,  1.3326e-03,\n        -6.9197e-04,  1.1402e-03,  9.5035e-04, -9.4477e-05,  6.1487e-04,\n        -1.0153e-03, -3.5232e-04, -1.0865e-03, -6.1240e-04,  1.3662e-03,\n         5.8401e-05, -1.4130e-03,  9.9795e-04, -8.2272e-04,  3.6306e-04,\n        -1.8050e-04, -8.1531e-04,  2.5188e-03, -6.1446e-04, -9.7560e-04,\n         1.8802e-04,  2.9791e-04, -4.7047e-04,  1.3389e-04,  3.2362e-05,\n        -8.4817e-04, -1.1369e-03]), 'meta_learner.transductive_net.self_attention.out_proj.weight': tensor([[ 0.0647, -0.0174, -0.0954,  ...,  0.0531,  0.1295, -0.0693],\n        [ 0.0807,  0.0200, -0.0795,  ...,  0.0535,  0.0938, -0.0477],\n        [-0.1139,  0.0650,  0.0710,  ..., -0.1022,  0.0518,  0.0433],\n        ...,\n        [-0.0014, -0.0696,  0.0625,  ..., -0.0371,  0.0228, -0.0878],\n        [-0.0978,  0.0126, -0.1109,  ..., -0.0802, -0.0673, -0.0976],\n        [ 0.1095, -0.0868, -0.0458,  ...,  0.0314, -0.0393, -0.0044]]), 'meta_learner.transductive_net.self_attention.out_proj.bias': tensor([ 9.3160e-04, -1.3670e-04,  4.5227e-04, -1.8607e-03,  4.8912e-05,\n         4.4433e-04,  5.3277e-05,  1.3728e-04,  4.9707e-04,  1.6656e-04,\n        -2.7355e-03, -3.3300e-04, -1.0398e-03,  4.1603e-04,  1.3860e-03,\n         1.3230e-03,  2.6119e-04, -4.8286e-04,  1.8451e-03,  3.3227e-04,\n         2.1986e-04, -2.2568e-03,  1.0540e-03,  6.9836e-04,  2.5122e-04,\n        -9.3472e-04, -6.6646e-04,  2.1454e-04, -1.0352e-03,  7.4807e-04,\n        -6.1665e-04, -1.3259e-03, -8.8353e-04, -3.7176e-04,  8.3463e-04,\n        -5.5058e-04, -1.7110e-03,  7.9191e-04,  1.3203e-03, -4.1408e-04,\n         1.1466e-03, -9.9645e-04,  5.4185e-04,  6.1418e-04,  3.0522e-04,\n        -4.5222e-04, -3.8191e-04, -2.0073e-04,  2.0663e-05,  4.6780e-04,\n        -1.9110e-03, -1.9966e-04,  1.4784e-04,  1.3483e-03, -4.0524e-04,\n        -1.2518e-04, -1.9867e-03,  2.9106e-04, -1.3126e-05, -4.5815e-04,\n         7.0781e-05, -2.9320e-04,  1.0197e-03,  1.0501e-03]), 'meta_learner.transductive_net.graph_conv.weight': tensor([[ 0.0630, -0.0885, -0.0087,  ...,  0.0595,  0.0228, -0.0685],\n        [ 0.0993, -0.1186, -0.0224,  ..., -0.0206,  0.0375, -0.1219],\n        [ 0.0575, -0.0640, -0.0260,  ...,  0.0789, -0.0564,  0.0113],\n        ...,\n        [ 0.0439, -0.0565, -0.0302,  ...,  0.1215,  0.1201,  0.1122],\n        [ 0.0219, -0.1014,  0.1182,  ..., -0.0222,  0.1147, -0.0791],\n        [-0.1208, -0.1128, -0.0077,  ..., -0.0572,  0.0240,  0.0366]]), 'meta_learner.transductive_net.graph_conv.bias': tensor([ 0.1129,  0.0705, -0.0254, -0.0198, -0.0667,  0.0024, -0.0163, -0.0188,\n        -0.0896, -0.0651,  0.1055, -0.0273, -0.0246,  0.0073, -0.0513,  0.0978,\n        -0.0067,  0.0496,  0.0247,  0.1243,  0.1149,  0.0299,  0.1190, -0.0572,\n        -0.0419,  0.0216, -0.0767,  0.0992,  0.0492, -0.1185,  0.0614, -0.0809,\n         0.0374,  0.0456,  0.0666, -0.0450, -0.0677,  0.0561, -0.0678,  0.0621,\n        -0.0598,  0.1221, -0.0775,  0.0065,  0.1133, -0.0553, -0.0043,  0.1035,\n        -0.1206, -0.0777, -0.0676,  0.0347,  0.0844,  0.1223,  0.1247, -0.0056,\n        -0.0112, -0.0825, -0.1192, -0.0576, -0.0924,  0.1123,  0.0925, -0.0151]), 'meta_learner.transductive_net.layer_norm.weight': tensor([0.9982, 1.0107, 0.9914, 0.9988, 0.9927, 0.9876, 0.9931, 0.9978, 0.9927,\n        0.9955, 0.9715, 0.9746, 0.9887, 0.9896, 0.9942, 0.9915, 0.9770, 0.9940,\n        0.9960, 0.9866, 1.0105, 0.9840, 1.0018, 0.9926, 0.9941, 1.0154, 0.9858,\n        0.9934, 1.0068, 0.9968, 1.0070, 0.9926, 1.0014, 1.0045, 0.9897, 0.9914,\n        1.0082, 1.0029, 1.0146, 0.9918, 0.9932, 1.0079, 0.9996, 0.9945, 0.9927,\n        0.9915, 0.9840, 0.9880, 0.9813, 1.0024, 0.9775, 0.9994, 0.9968, 1.0130,\n        0.9903, 0.9864, 0.9970, 0.9980, 1.0013, 1.0065, 1.0004, 0.9961, 1.0010,\n        0.9849]), 'meta_learner.transductive_net.layer_norm.bias': tensor([-1.1433e-02, -4.0813e-03,  1.2077e-03,  4.9871e-03, -7.9680e-03,\n         5.1240e-03, -6.3538e-04,  4.9295e-03,  5.3529e-03,  2.2205e-03,\n        -1.4026e-02, -3.8475e-03, -1.3318e-02, -3.5203e-03,  1.0536e-02,\n        -1.1529e-02,  2.1261e-03,  5.0953e-03, -4.9800e-04, -8.9676e-03,\n        -1.1638e-02, -1.1845e-02,  5.7176e-03, -1.9229e-02,  4.9848e-03,\n         3.4789e-03,  8.8072e-03,  6.9768e-03,  1.4840e-02,  9.6129e-03,\n        -6.4369e-03,  7.2475e-03,  1.6271e-02, -1.0237e-02,  2.4584e-03,\n         1.0042e-02, -1.7320e-03,  3.0482e-03, -2.4053e-05,  4.6008e-03,\n        -8.2895e-03,  2.2425e-04,  1.3074e-02, -1.1741e-02,  2.1648e-03,\n         4.4716e-03, -1.6328e-03, -7.8810e-03, -7.4105e-03, -5.9879e-03,\n         2.8486e-03,  2.1693e-02, -1.3169e-03, -4.6231e-03, -3.3041e-04,\n        -1.4261e-03, -1.3820e-02,  5.5190e-03, -1.3303e-02,  6.2975e-03,\n        -1.1451e-02, -1.2695e-02, -6.6337e-04, -1.6869e-03])}, sample_count=46300, training_loss=np.float64(0.06484403490192361), validation_accuracy=np.float64(0.916666673289405), validation_precision=0.0, validation_recall=0.0, validation_f1_score=0.0, timestamp=1760153437.9003963, model_hash='4b047170874d62a20a271ee716f41f160d18dbf5dd59dbdb407d8e1aa68b9ee3', ipfs_cid=None, blockchain_tx_hash=None)",
        "ClientUpdate(client_id='client_3', model_parameters={'meta_learner.transductive_net.feature_extractors.0.0.weight': tensor([[-0.1686, -0.1072, -0.1187,  ...,  0.1694, -0.0004, -0.1745],\n        [ 0.0864,  0.0730, -0.0999,  ...,  0.0074, -0.1471,  0.0216],\n        [ 0.0241, -0.0511,  0.0405,  ...,  0.1028, -0.0065, -0.1052],\n        ...,\n        [ 0.1116, -0.0265, -0.1013,  ...,  0.1212,  0.1059,  0.1531],\n        [ 0.0040,  0.1403, -0.1683,  ...,  0.0973, -0.0618,  0.1391],\n        [-0.0368,  0.0632,  0.0665,  ...,  0.1126, -0.0277,  0.0073]]), 'meta_learner.transductive_net.feature_extractors.0.0.bias': tensor([ 0.1645, -0.0300, -0.1477,  0.1418, -0.0612,  0.0999,  0.1494,  0.0361,\n        -0.0441, -0.0760,  0.1103,  0.0721, -0.0271, -0.0598, -0.1295, -0.0938,\n         0.0032,  0.0416, -0.0118,  0.1618,  0.0675,  0.0327,  0.0707, -0.0320,\n         0.0513,  0.0835,  0.0415,  0.0880,  0.0812, -0.0133, -0.0131,  0.1288,\n         0.1065, -0.0029, -0.0366,  0.1259, -0.0179, -0.1738, -0.0893,  0.1427,\n        -0.0487, -0.0221,  0.1494, -0.0128, -0.1186, -0.1136, -0.1151,  0.1523,\n        -0.1296,  0.0197, -0.1052, -0.1057,  0.0551,  0.1653, -0.0223, -0.0620,\n        -0.0192,  0.0854, -0.1156,  0.0347,  0.0047, -0.1402, -0.0207, -0.1517,\n        -0.0797, -0.0945, -0.0521,  0.0744, -0.1032, -0.1240, -0.0752, -0.0565,\n        -0.0564, -0.0413, -0.1349,  0.1862,  0.1370, -0.1191, -0.0107, -0.1530,\n         0.0257,  0.0229,  0.1348,  0.0712,  0.0761, -0.0883,  0.1197, -0.1025,\n         0.0723,  0.1166, -0.1536, -0.1299,  0.0601,  0.0137,  0.1222, -0.1541,\n         0.0145, -0.1624, -0.0870,  0.1700,  0.0593,  0.1151, -0.1334,  0.1692,\n        -0.0789,  0.0563, -0.1272,  0.1462,  0.1394, -0.0140, -0.1875,  0.0633,\n        -0.0885,  0.0587,  0.0046,  0.0724,  0.1366, -0.1160, -0.0353,  0.1693,\n        -0.1383, -0.1754,  0.1692,  0.1003, -0.0653, -0.1824, -0.1067,  0.0471]), 'meta_learner.transductive_net.feature_extractors.1.0.weight': tensor([[-0.0457, -0.0462,  0.0414,  ..., -0.1541, -0.0453, -0.0019],\n        [-0.1919, -0.1066, -0.0407,  ...,  0.1128,  0.1716, -0.0107],\n        [-0.0093, -0.0946,  0.1607,  ..., -0.0082, -0.1408, -0.0619],\n        ...,\n        [-0.1595, -0.0658,  0.1477,  ...,  0.1787,  0.1301, -0.0092],\n        [ 0.1316,  0.1174,  0.0965,  ..., -0.1512,  0.1209,  0.0710],\n        [ 0.0429,  0.0131,  0.0619,  ..., -0.0595, -0.0971, -0.0443]]), 'meta_learner.transductive_net.feature_extractors.1.0.bias': tensor([-0.0074, -0.1384, -0.0685,  0.1593, -0.0029, -0.0666,  0.1311,  0.0155,\n        -0.0543, -0.0006, -0.0792, -0.1506, -0.0151,  0.0960, -0.1064, -0.0965,\n         0.0346, -0.1872, -0.0450,  0.0115, -0.0615,  0.0234,  0.1742, -0.0961,\n         0.1477, -0.0544, -0.0927, -0.0062,  0.1607, -0.1086,  0.0395, -0.1842,\n         0.0849, -0.1478, -0.1049,  0.0719,  0.0998, -0.1083, -0.0589, -0.1407,\n        -0.0711, -0.0121, -0.1495,  0.0343, -0.0650, -0.1110,  0.1291, -0.1608,\n         0.0366, -0.1822,  0.1419,  0.0382,  0.0858, -0.0365, -0.0133, -0.0380,\n         0.0613, -0.0666,  0.0519, -0.0039, -0.0070, -0.0098,  0.1663,  0.1347]), 'meta_learner.transductive_net.feature_extractors.2.0.weight': tensor([[-0.0787, -0.0059, -0.0558,  ..., -0.1292,  0.0894, -0.0692],\n        [ 0.1714,  0.0659,  0.0789,  ..., -0.1406,  0.0770, -0.0405],\n        [ 0.1388,  0.0987, -0.1194,  ...,  0.0983, -0.1743, -0.0954],\n        ...,\n        [-0.0344,  0.0900,  0.1537,  ..., -0.1675,  0.0668, -0.0772],\n        [ 0.1620, -0.0009,  0.1124,  ..., -0.0401,  0.0933,  0.1526],\n        [ 0.0780,  0.0281,  0.0983,  ...,  0.0904, -0.1442, -0.0218]]), 'meta_learner.transductive_net.feature_extractors.2.0.bias': tensor([ 0.0690,  0.0431,  0.1622, -0.0046,  0.1136,  0.0968,  0.1804, -0.1343,\n        -0.1318, -0.0018,  0.0717, -0.0804, -0.1265,  0.1321, -0.0086,  0.0271,\n         0.0246, -0.1438,  0.1518, -0.1748, -0.0634,  0.1215,  0.1043,  0.0109,\n         0.0976,  0.1303,  0.0645, -0.0376, -0.1658,  0.1775, -0.0710,  0.1419,\n         0.0799,  0.0736, -0.1156,  0.0999, -0.1591,  0.0597,  0.0012, -0.1731,\n         0.1450, -0.1637,  0.0854,  0.1381,  0.1328, -0.0265, -0.0537, -0.0983,\n         0.0503, -0.0928,  0.0479, -0.0441,  0.1622,  0.1604, -0.1633,  0.0188,\n        -0.0706, -0.0890,  0.0730, -0.0613, -0.0962, -0.0199,  0.0560, -0.0940,\n        -0.1091,  0.0457, -0.0327, -0.0517, -0.0298,  0.0535, -0.1560, -0.1125,\n        -0.0819,  0.1042,  0.0913, -0.1365,  0.0840, -0.0100,  0.1305,  0.0882,\n        -0.0799, -0.1829, -0.1530, -0.0420,  0.1539, -0.1109,  0.0258,  0.1824,\n        -0.1310,  0.0998,  0.1300, -0.1168, -0.0975, -0.0023, -0.0523,  0.0180,\n         0.1405,  0.0496,  0.1285, -0.0925, -0.0245,  0.0212,  0.0945,  0.0257,\n         0.0160, -0.1149, -0.0703, -0.1043,  0.0741,  0.0173, -0.0637, -0.0548,\n         0.0716,  0.0357, -0.0306, -0.0316, -0.0383, -0.0300, -0.0171,  0.0235,\n         0.0152,  0.0863, -0.0063,  0.0938,  0.0043, -0.0934,  0.0861,  0.0321,\n        -0.1777,  0.1170,  0.1227, -0.1543, -0.1329, -0.0637,  0.0451,  0.0616,\n        -0.0577, -0.1240,  0.0592, -0.0528, -0.0258,  0.0342, -0.1804,  0.0004,\n        -0.1043,  0.0167,  0.0531, -0.0648,  0.0762, -0.0533, -0.0510,  0.1158,\n        -0.0367, -0.0407, -0.0534,  0.0985,  0.0373, -0.1058,  0.1395,  0.1152,\n         0.0614, -0.1436, -0.0564, -0.0228,  0.0158, -0.0963, -0.1339, -0.0125,\n         0.0685, -0.0206,  0.0018,  0.0342, -0.0443,  0.0922, -0.0641,  0.1373,\n        -0.0343,  0.1162,  0.0950, -0.1284, -0.0059,  0.0089, -0.1562,  0.0345,\n        -0.1017,  0.0524, -0.0400,  0.0903,  0.0099, -0.1402, -0.0486, -0.0198,\n         0.0705,  0.1071, -0.1641,  0.1170, -0.1357,  0.0295,  0.1008,  0.0958,\n         0.0797,  0.0577, -0.0931, -0.0046, -0.0184,  0.0503,  0.1357,  0.1502,\n        -0.1526,  0.0179, -0.1638, -0.1419,  0.0513, -0.0253,  0.0709,  0.1742,\n         0.1321,  0.1729, -0.0921,  0.0117, -0.0974,  0.1415, -0.0574,  0.0773,\n        -0.0782,  0.0631,  0.0769, -0.1312,  0.1055, -0.0780,  0.1141, -0.1698,\n         0.1577,  0.0206, -0.0651, -0.1380, -0.0317,  0.0564,  0.1710, -0.0749,\n         0.1859,  0.0486,  0.0984, -0.0903, -0.0578, -0.1281,  0.0401,  0.0321,\n        -0.0887,  0.1532,  0.0894,  0.1609, -0.1207,  0.0395, -0.1532, -0.1158]), 'meta_learner.transductive_net.feature_projection.0.weight': tensor([[-0.0313, -0.0366, -0.0367,  ..., -0.0040, -0.0289,  0.0320],\n        [-0.0337,  0.0339,  0.0557,  ...,  0.0152,  0.0368, -0.0454],\n        [-0.0405,  0.0361,  0.0475,  ...,  0.0444,  0.0477,  0.0075],\n        ...,\n        [-0.0204, -0.0052,  0.0495,  ...,  0.0136, -0.0297,  0.0370],\n        [-0.0095, -0.0129,  0.0027,  ...,  0.0317, -0.0130,  0.0008],\n        [-0.0538,  0.0069,  0.0133,  ...,  0.0102,  0.0311,  0.0410]]), 'meta_learner.transductive_net.feature_projection.0.bias': tensor([-0.0214, -0.0164, -0.0208, -0.0419, -0.0310, -0.0041,  0.0147,  0.0437,\n        -0.0418,  0.0422, -0.0168,  0.0241, -0.0288,  0.0121,  0.0336,  0.0302,\n        -0.0380, -0.0093, -0.0011,  0.0131, -0.0188,  0.0326,  0.0208, -0.0067,\n         0.0174,  0.0013,  0.0317,  0.0060, -0.0035,  0.0143,  0.0353, -0.0473,\n         0.0122, -0.0389,  0.0288,  0.0039, -0.0070,  0.0305, -0.0119,  0.0327,\n        -0.0277, -0.0128,  0.0341, -0.0505, -0.0251, -0.0281,  0.0088,  0.0219,\n        -0.0166, -0.0247,  0.0227,  0.0031, -0.0221, -0.0272, -0.0241,  0.0157,\n         0.0129,  0.0546, -0.0309,  0.0005,  0.0383,  0.0276,  0.0042,  0.0091]), 'meta_learner.transductive_net.adaptive_net.0.weight': tensor([[ 0.1193,  0.0529, -0.0517,  ..., -0.0253,  0.1239,  0.0198],\n        [ 0.0265,  0.0626,  0.1006,  ...,  0.0291,  0.0271, -0.0700],\n        [-0.0209, -0.0927,  0.0299,  ..., -0.1143,  0.0925,  0.0052],\n        ...,\n        [ 0.0357,  0.0575, -0.0909,  ...,  0.1232,  0.0481,  0.0861],\n        [-0.0811,  0.0515, -0.0042,  ..., -0.0685,  0.0764,  0.0191],\n        [-0.1022,  0.0405, -0.0620,  ...,  0.0118, -0.0874, -0.1000]]), 'meta_learner.transductive_net.adaptive_net.0.bias': tensor([ 0.0257, -0.0376,  0.0998, -0.0965, -0.0031,  0.0878,  0.0009, -0.0171,\n         0.0412,  0.1012,  0.0374,  0.0310, -0.1149, -0.0162, -0.0301, -0.0591,\n        -0.0017, -0.0988, -0.0060, -0.0819,  0.0105,  0.0675, -0.0388,  0.1213,\n         0.1240,  0.1104,  0.1097,  0.0882, -0.0878,  0.0018,  0.0863,  0.0531,\n         0.0818, -0.0027,  0.0711, -0.0884,  0.0743, -0.0356,  0.0094, -0.0885,\n         0.1007, -0.1105,  0.0558,  0.0502, -0.0063, -0.0536,  0.0053, -0.0090,\n         0.0697,  0.0718, -0.0794,  0.1215, -0.0132,  0.0330,  0.1205,  0.1167,\n         0.0538, -0.0806, -0.0976,  0.0452,  0.0914, -0.0630,  0.0727, -0.1167]), 'meta_learner.transductive_net.adaptive_net.3.weight': tensor([[ 0.0267, -0.0095, -0.0381,  ..., -0.0233, -0.0358,  0.0518],\n        [-0.0926, -0.1118,  0.0376,  ..., -0.0237, -0.0399, -0.0786],\n        [ 0.0115,  0.1149,  0.0311,  ..., -0.0566, -0.1173,  0.0006],\n        ...,\n        [ 0.0392,  0.0522,  0.0266,  ...,  0.0399, -0.1193, -0.0253],\n        [-0.0526, -0.0565,  0.1239,  ...,  0.0376, -0.0866, -0.0481],\n        [-0.0638, -0.0378, -0.0068,  ..., -0.0936, -0.0683,  0.0016]]), 'meta_learner.transductive_net.adaptive_net.3.bias': tensor([ 0.0455,  0.0498,  0.0815,  0.0566,  0.0293,  0.0274, -0.0471, -0.0018,\n        -0.0409,  0.1197, -0.0750, -0.0763,  0.0136, -0.0129,  0.0783,  0.1193,\n        -0.1174,  0.0922,  0.0346,  0.0998, -0.1063,  0.0552, -0.1232,  0.0120,\n         0.0281,  0.0188, -0.1219, -0.0805, -0.0008,  0.0542, -0.0874,  0.0765]), 'meta_learner.transductive_net.adaptive_net.6.weight': tensor([[-0.0385,  0.0673,  0.1108,  0.0979,  0.1496, -0.0716, -0.0968,  0.1446,\n          0.0860,  0.1473, -0.1305, -0.1032,  0.1742,  0.0291,  0.0070,  0.1329,\n         -0.0405,  0.0421, -0.1247, -0.1079,  0.1128,  0.1089,  0.0694, -0.0664,\n         -0.1346, -0.0756,  0.0618, -0.1395,  0.1121,  0.0712, -0.0533, -0.1398],\n        [-0.0184, -0.0519,  0.0560, -0.1062, -0.0953, -0.1454, -0.0826, -0.1568,\n         -0.0240, -0.1253, -0.0668, -0.0187,  0.0552, -0.0243,  0.1226,  0.1348,\n         -0.1254, -0.0788, -0.0334,  0.1395, -0.1063,  0.1677,  0.0629,  0.1035,\n          0.1455,  0.0221, -0.0491, -0.0642, -0.0795,  0.0691,  0.0766,  0.0586]]), 'meta_learner.transductive_net.adaptive_net.6.bias': tensor([ 0.1162, -0.0662]), 'meta_learner.transductive_net.attention_net.in_proj_weight': tensor([[ 0.1499, -0.1297, -0.0400,  ...,  0.0069,  0.1297, -0.0316],\n        [ 0.0651,  0.1326, -0.1040,  ..., -0.1450,  0.0255, -0.0074],\n        [ 0.0912, -0.0046,  0.1084,  ...,  0.0344,  0.0308,  0.0787],\n        ...,\n        [ 0.0304,  0.0523, -0.0867,  ...,  0.0757, -0.0210,  0.1175],\n        [-0.0734,  0.1009, -0.0293,  ..., -0.1144,  0.0155, -0.0095],\n        [ 0.1256, -0.0447,  0.0847,  ...,  0.1312, -0.0769, -0.1114]]), 'meta_learner.transductive_net.attention_net.in_proj_bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'meta_learner.transductive_net.attention_net.out_proj.weight': tensor([[ 0.0873, -0.0201, -0.0694,  ...,  0.0288, -0.0274,  0.0071],\n        [ 0.0220, -0.0683, -0.0115,  ...,  0.0162,  0.0030,  0.0250],\n        [ 0.0620, -0.0258, -0.0088,  ..., -0.0979,  0.1243,  0.0966],\n        ...,\n        [-0.0169,  0.1037, -0.0049,  ..., -0.0238, -0.0969, -0.0654],\n        [ 0.0961, -0.0908,  0.0204,  ...,  0.0248,  0.1000,  0.0646],\n        [-0.1153, -0.0332, -0.0012,  ..., -0.0973, -0.1118, -0.0163]]), 'meta_learner.transductive_net.attention_net.out_proj.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'meta_learner.transductive_net.self_attention.in_proj_weight': tensor([[ 0.0229,  0.1506, -0.1286,  ...,  0.1271, -0.0374, -0.0128],\n        [-0.1366, -0.0675,  0.0014,  ...,  0.0151, -0.1456,  0.0227],\n        [ 0.0877,  0.0626,  0.0646,  ..., -0.0577,  0.0970,  0.1491],\n        ...,\n        [-0.0902, -0.1331, -0.0252,  ...,  0.1556, -0.0699, -0.1114],\n        [-0.1189,  0.0715,  0.0695,  ...,  0.1074, -0.0775, -0.1488],\n        [-0.0558, -0.0052, -0.0074,  ..., -0.0073,  0.0217,  0.0401]]), 'meta_learner.transductive_net.self_attention.in_proj_bias': tensor([-3.7212e-03,  3.0916e-03, -3.7516e-03,  5.3551e-03,  1.6061e-03,\n         7.8315e-03,  1.1060e-02,  9.5802e-03,  2.3521e-04, -6.9842e-03,\n        -1.6301e-02, -8.7294e-03,  8.4727e-03,  4.3676e-03,  1.2648e-02,\n         8.6566e-03, -1.0508e-02, -1.8254e-03,  6.8667e-03,  1.0593e-02,\n         9.6963e-03, -1.4413e-02, -2.2026e-02, -4.9430e-03,  1.6225e-02,\n        -1.0747e-02, -5.9073e-03, -1.3240e-02,  3.9923e-03,  4.0965e-03,\n        -9.5300e-03,  2.7162e-03,  2.6814e-03, -5.3692e-03, -2.0125e-02,\n         2.9685e-03,  3.6997e-03, -6.7761e-03, -1.3388e-03,  1.0284e-03,\n        -6.9335e-04,  4.6229e-03,  8.6701e-03,  3.0372e-03, -1.1092e-02,\n        -7.6529e-04,  2.5719e-03, -1.8033e-03, -1.5663e-02, -6.7322e-03,\n         1.3128e-02, -6.7403e-03, -1.7672e-02, -8.8333e-03, -1.0276e-03,\n        -1.0114e-02, -6.6568e-03, -1.1511e-02, -9.9616e-03, -8.7302e-03,\n         6.1510e-03,  4.8202e-03,  1.0937e-02, -1.6348e-02,  3.5682e-06,\n        -1.3191e-05,  1.3791e-04,  2.9817e-05, -8.3256e-05, -1.1492e-05,\n        -8.2101e-05,  2.3548e-04,  7.3568e-05, -7.0920e-05, -1.8439e-05,\n         1.4311e-04, -4.7202e-05, -5.1805e-06, -3.5463e-05,  1.4634e-05,\n        -7.1274e-05, -1.2180e-04, -6.6796e-06, -9.4612e-05, -2.8069e-05,\n        -1.0053e-05, -2.2906e-05, -1.1975e-04,  6.3267e-05, -2.1836e-05,\n        -5.5685e-05,  1.6642e-04, -2.1851e-05, -1.5241e-04, -1.2846e-08,\n        -2.1245e-05, -1.2894e-04, -3.4268e-05, -5.7883e-05,  4.5155e-05,\n         4.7920e-05,  5.3792e-05,  1.2273e-06,  6.4387e-05, -3.1902e-05,\n         1.2098e-04,  1.4749e-05, -6.0390e-05,  4.1293e-05, -2.2776e-05,\n        -9.0611e-05, -2.4952e-06,  2.6941e-06, -1.3781e-05, -1.2354e-06,\n        -1.7550e-05,  2.0431e-05,  3.6031e-05,  8.5614e-05,  1.0156e-04,\n         2.7885e-05,  3.6748e-05, -1.4051e-04, -3.8926e-05, -3.0371e-05,\n         1.1302e-04,  9.2011e-05, -6.0922e-05,  1.1082e-04,  1.4241e-03,\n        -8.4088e-04,  1.5136e-04,  1.0675e-03,  7.1682e-04,  1.7155e-05,\n        -7.0670e-04,  9.6929e-05,  3.0412e-04, -5.5577e-04, -2.5319e-04,\n         1.1090e-03,  1.4486e-03,  5.5765e-05,  7.5148e-04,  3.1867e-04,\n        -8.7925e-05, -2.7303e-04,  3.9059e-04,  1.3532e-03,  1.2577e-03,\n        -7.9624e-04, -1.0509e-03,  1.0393e-03, -1.1016e-03,  7.1724e-04,\n        -1.5623e-03, -7.3162e-04, -5.7077e-04, -5.4874e-04, -1.7010e-03,\n         1.8353e-03, -2.0378e-03,  4.1107e-04, -3.4463e-04,  6.9297e-04,\n        -2.0955e-04, -3.1885e-04, -1.6917e-03, -1.6158e-04, -5.6066e-04,\n        -3.9128e-04, -2.1235e-03,  1.1947e-03,  1.5574e-03, -1.5049e-03,\n        -8.3666e-04, -6.5023e-05, -3.7012e-04,  1.0642e-03, -1.5258e-03,\n         6.3459e-04, -1.5576e-03, -5.3270e-04,  7.3531e-04,  3.4227e-04,\n        -3.4470e-04, -7.5426e-04,  7.5541e-05, -1.2015e-03,  9.6948e-04,\n         1.4097e-03, -1.5639e-03]), 'meta_learner.transductive_net.self_attention.out_proj.weight': tensor([[ 0.0590,  0.0170, -0.0910,  ...,  0.0340,  0.1222, -0.0994],\n        [ 0.1012,  0.0052, -0.0808,  ...,  0.0850,  0.1226, -0.0418],\n        [-0.1078,  0.0507,  0.0812,  ..., -0.1035,  0.0680,  0.0545],\n        ...,\n        [-0.0089, -0.0917,  0.0665,  ..., -0.0478,  0.0230, -0.0694],\n        [-0.0791,  0.0294, -0.1273,  ..., -0.0588, -0.0925, -0.0931],\n        [ 0.1121, -0.0747, -0.0543,  ...,  0.0436, -0.0447,  0.0061]]), 'meta_learner.transductive_net.self_attention.out_proj.bias': tensor([-1.5970e-04,  1.9947e-03, -1.6768e-04, -3.0870e-04,  3.3927e-05,\n        -7.3656e-04,  2.3050e-04,  5.4673e-04, -3.8125e-04, -2.3630e-03,\n        -8.6755e-04, -4.5110e-04, -2.2338e-04, -9.2078e-05, -1.4816e-03,\n        -5.9589e-06, -2.6341e-03, -9.9163e-04, -6.3919e-04, -4.7276e-04,\n         5.5237e-04, -5.9708e-04, -3.5568e-04,  1.5005e-03,  1.1407e-04,\n        -5.9773e-04, -5.1822e-04, -3.2799e-05,  2.6959e-04,  2.4688e-05,\n         5.5254e-04,  1.8221e-04,  1.9789e-04,  2.2131e-04, -2.9630e-04,\n         3.3560e-04,  4.3840e-04, -1.0635e-03,  4.8312e-04, -1.7063e-03,\n        -1.2676e-03,  5.9698e-04, -2.0193e-04,  5.1899e-04,  5.1030e-04,\n         6.4261e-04, -4.3703e-04, -1.4072e-04, -6.4180e-04,  2.0785e-03,\n        -2.7852e-04,  3.4579e-04, -4.0007e-04, -1.7512e-03,  1.4053e-04,\n         1.3618e-03,  1.0641e-03, -7.1703e-04, -1.1596e-03, -1.7520e-04,\n         4.6240e-04, -1.1876e-03,  6.6628e-05, -6.5534e-05]), 'meta_learner.transductive_net.graph_conv.weight': tensor([[ 0.0630, -0.0885, -0.0087,  ...,  0.0595,  0.0228, -0.0685],\n        [ 0.0993, -0.1186, -0.0224,  ..., -0.0206,  0.0375, -0.1219],\n        [ 0.0575, -0.0640, -0.0260,  ...,  0.0789, -0.0564,  0.0113],\n        ...,\n        [ 0.0439, -0.0565, -0.0302,  ...,  0.1215,  0.1201,  0.1122],\n        [ 0.0219, -0.1014,  0.1182,  ..., -0.0222,  0.1147, -0.0791],\n        [-0.1208, -0.1128, -0.0077,  ..., -0.0572,  0.0240,  0.0366]]), 'meta_learner.transductive_net.graph_conv.bias': tensor([ 0.1129,  0.0705, -0.0254, -0.0198, -0.0667,  0.0024, -0.0163, -0.0188,\n        -0.0896, -0.0651,  0.1055, -0.0273, -0.0246,  0.0073, -0.0513,  0.0978,\n        -0.0067,  0.0496,  0.0247,  0.1243,  0.1149,  0.0299,  0.1190, -0.0572,\n        -0.0419,  0.0216, -0.0767,  0.0992,  0.0492, -0.1185,  0.0614, -0.0809,\n         0.0374,  0.0456,  0.0666, -0.0450, -0.0677,  0.0561, -0.0678,  0.0621,\n        -0.0598,  0.1221, -0.0775,  0.0065,  0.1133, -0.0553, -0.0043,  0.1035,\n        -0.1206, -0.0777, -0.0676,  0.0347,  0.0844,  0.1223,  0.1247, -0.0056,\n        -0.0112, -0.0825, -0.1192, -0.0576, -0.0924,  0.1123,  0.0925, -0.0151]), 'meta_learner.transductive_net.layer_norm.weight': tensor([0.9912, 0.9942, 1.0099, 0.9930, 0.9903, 0.9903, 1.0037, 1.0058, 0.9925,\n        0.9863, 1.0021, 0.9951, 0.9822, 1.0055, 0.9877, 1.0012, 0.9962, 0.9907,\n        0.9962, 0.9937, 0.9961, 0.9933, 1.0033, 0.9890, 0.9956, 1.0175, 0.9961,\n        0.9887, 0.9999, 0.9906, 0.9918, 0.9936, 1.0049, 0.9894, 1.0021, 1.0057,\n        1.0011, 1.0053, 0.9903, 0.9894, 0.9999, 1.0011, 0.9843, 0.9918, 0.9898,\n        0.9855, 1.0149, 0.9972, 0.9997, 0.9895, 0.9943, 0.9919, 1.0100, 0.9997,\n        0.9921, 0.9950, 1.0022, 0.9828, 1.0095, 1.0055, 0.9936, 0.9966, 0.9822,\n        0.9936]), 'meta_learner.transductive_net.layer_norm.bias': tensor([ 0.0192, -0.0008,  0.0067,  0.0062, -0.0006, -0.0085, -0.0096, -0.0151,\n        -0.0147, -0.0250, -0.0006, -0.0057,  0.0019,  0.0012, -0.0074,  0.0036,\n        -0.0032, -0.0104, -0.0024, -0.0088,  0.0020,  0.0078, -0.0127,  0.0062,\n        -0.0020, -0.0024,  0.0117,  0.0107,  0.0012, -0.0075,  0.0057,  0.0114,\n        -0.0127, -0.0113,  0.0018, -0.0024,  0.0087, -0.0049, -0.0039, -0.0033,\n         0.0030, -0.0106, -0.0036,  0.0114,  0.0098,  0.0098, -0.0047, -0.0138,\n        -0.0067,  0.0052, -0.0007, -0.0006,  0.0028,  0.0003,  0.0069,  0.0101,\n         0.0054, -0.0076,  0.0129, -0.0115, -0.0071, -0.0101, -0.0057, -0.0008])}, sample_count=93499, training_loss=np.float64(0.0934542855868737), validation_accuracy=np.float64(0.8766666875945197), validation_precision=0.0, validation_recall=0.0, validation_f1_score=0.0, timestamp=1760153438.2698488, model_hash='64fcab42b0e0c649a25af97efe429b4acc242fec738c371699c99d468a023c38', ipfs_cid=None, blockchain_tx_hash=None)"
      ],
      "validation_metrics": {
        "loss": 0.7187393307685852,
        "accuracy": 0.329,
        "f1_score": 0.16289089541008278,
        "samples": 1000
      }
    }
  ],
  "evaluation_results": {
    "base_model": {
      "accuracy": 0.797,
      "precision": 0.803680981595092,
      "recall": 0.786,
      "f1_score": 0.7969754340275174,
      "mccc": 0.5941438002015791,
      "zero_day_detection_rate": 0.5,
      "optimal_threshold": "0.48523298",
      "roc_auc": 0.852631,
      "confusion_matrix": {
        "tn": 808,
        "fp": 192,
        "fn": 214,
        "tp": 786
      },
      "test_samples": 2000,
      "query_samples": 2000,
      "support_samples": 500
    },
    "ttt_model": {
      "accuracy": 0.94,
      "precision": 0.9923076923076923,
      "recall": 0.9020979020979021,
      "f1_score": 0.945054945054945,
      "mccc": 0.8841551737304516,
      "zero_day_detection_rate": 0.572,
      "avg_confidence": "0.5159004",
      "confusion_matrix": {
        "tn": 106,
        "fp": 1,
        "fn": 14,
        "tp": 129
      },
      "support_samples": 250,
      "query_samples": 250,
      "ttt_adaptation_steps": 10,
      "optimal_threshold": 0.5,
      "roc_auc": 0.9812430560094112,
      "roc_curve": {
        "fpr": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.009345794392523364,
          0.009345794392523364,
          0.018691588785046728,
          0.018691588785046728,
          0.037383177570093455,
          0.037383177570093455,
          0.056074766355140186,
          0.056074766355140186,
          0.21495327102803738,
          0.21495327102803738,
          0.24299065420560748,
          0.24299065420560748,
          0.9345794392523364,
          0.9345794392523364,
          0.9439252336448598,
          0.9439252336448598,
          1.0
        ],
        "tpr": [
          0.0,
          0.006993006993006993,
          0.013986013986013986,
          0.027972027972027972,
          0.03496503496503497,
          0.04895104895104895,
          0.0979020979020979,
          0.11188811188811189,
          0.11888111888111888,
          0.13986013986013987,
          0.15384615384615385,
          0.18181818181818182,
          0.1958041958041958,
          0.2097902097902098,
          0.22377622377622378,
          0.23076923076923078,
          0.2517482517482518,
          0.27972027972027974,
          0.2937062937062937,
          0.34265734265734266,
          0.3706293706293706,
          0.42657342657342656,
          0.45454545454545453,
          0.46853146853146854,
          0.4755244755244755,
          0.4965034965034965,
          0.5034965034965035,
          0.5174825174825175,
          0.5384615384615384,
          0.5454545454545454,
          0.5874125874125874,
          0.6083916083916084,
          0.6223776223776224,
          0.6573426573426573,
          0.6853146853146853,
          0.7902097902097902,
          0.7902097902097902,
          0.9370629370629371,
          0.9370629370629371,
          0.958041958041958,
          0.958041958041958,
          0.965034965034965,
          0.965034965034965,
          0.972027972027972,
          0.972027972027972,
          0.9790209790209791,
          0.9790209790209791,
          0.986013986013986,
          0.986013986013986,
          0.993006993006993,
          0.993006993006993,
          1.0,
          1.0
        ],
        "thresholds": [
          Infinity,
          0.9997922778129578,
          0.9997833371162415,
          0.9997828602790833,
          0.9997826218605042,
          0.9997816681861877,
          0.9997792840003967,
          0.9997789263725281,
          0.999778687953949,
          0.9997785687446594,
          0.9997783303260803,
          0.9997780919075012,
          0.9997778534889221,
          0.999777615070343,
          0.9997771382331848,
          0.9997770190238953,
          0.9997768998146057,
          0.9997767806053162,
          0.999776303768158,
          0.9997761845588684,
          0.9997760653495789,
          0.9997753500938416,
          0.9997749924659729,
          0.9997748732566833,
          0.9997747540473938,
          0.9997743964195251,
          0.9997740387916565,
          0.9997739195823669,
          0.9997738003730774,
          0.9997736811637878,
          0.9997733235359192,
          0.9997732043266296,
          0.9997730851173401,
          0.9997729659080505,
          0.9997726082801819,
          0.9939708709716797,
          0.9934476017951965,
          0.004217830486595631,
          0.0036550508812069893,
          0.0034733405336737633,
          0.002319490537047386,
          0.002305347006767988,
          0.001804431900382042,
          0.0017324489308521152,
          0.0009137592278420925,
          0.00090656743850559,
          0.0007911922875791788,
          0.0007909280247986317,
          0.0003023625467903912,
          0.00029421510407701135,
          0.0002909076283685863,
          0.0002817097119987011,
          0.0002370233996771276
        ]
      }
    },
    "base_model_kfold": {
      "accuracy_mean": 0.7216923453167033,
      "accuracy_std": 0.004575271932959108,
      "precision_mean": 0.7216923453167033,
      "precision_std": 0.004575271932959108,
      "recall_mean": 0.7216923453167033,
      "recall_std": 0.004575271932959108,
      "macro_f1_mean": 0.7216653363844031,
      "macro_f1_std": 0.004559908056481843,
      "mcc_mean": 0.4434726435233936,
      "mcc_std": 0.009202781921797595,
      "confusion_matrix": [
        [
          978,
          385
        ],
        [
          387,
          976
        ]
      ],
      "roc_curve": {
        "fpr": [
          0.0,
          0.0,
          0.0,
          0.0007336757153338225,
          0.0007336757153338225,
          0.001467351430667645,
          0.001467351430667645,
          0.0022010271460014674,
          0.0022010271460014674,
          0.00293470286133529,
          0.00293470286133529,
          0.003668378576669112,
          0.003668378576669112,
          0.004402054292002935,
          0.004402054292002935,
          0.005135730007336757,
          0.005135730007336757,
          0.00586940572267058,
          0.00586940572267058,
          0.006603081438004402,
          0.006603081438004402,
          0.006603081438004402,
          0.006603081438004402,
          0.007336757153338224,
          0.007336757153338224,
          0.008070432868672046,
          0.008070432868672046,
          0.008070432868672046,
          0.008070432868672046,
          0.00880410858400587,
          0.00880410858400587,
          0.009537784299339692,
          0.009537784299339692,
          0.010271460014673514,
          0.010271460014673514,
          0.011005135730007337,
          0.011005135730007337,
          0.01173881144534116,
          0.01173881144534116,
          0.012472487160674981,
          0.012472487160674981,
          0.013206162876008804,
          0.013206162876008804,
          0.013939838591342627,
          0.013939838591342627,
          0.014673514306676448,
          0.014673514306676448,
          0.014673514306676448,
          0.014673514306676448,
          0.015407190022010272,
          0.015407190022010272,
          0.016140865737344093,
          0.016140865737344093,
          0.016874541452677916,
          0.016874541452677916,
          0.01760821716801174,
          0.01760821716801174,
          0.018341892883345562,
          0.018341892883345562,
          0.019075568598679385,
          0.019075568598679385,
          0.019809244314013204,
          0.019809244314013204,
          0.020542920029347028,
          0.020542920029347028,
          0.02127659574468085,
          0.02127659574468085,
          0.022010271460014674,
          0.022010271460014674,
          0.022743947175348497,
          0.022743947175348497,
          0.02347762289068232,
          0.02347762289068232,
          0.02347762289068232,
          0.02347762289068232,
          0.02421129860601614,
          0.02421129860601614,
          0.024944974321349962,
          0.024944974321349962,
          0.025678650036683785,
          0.025678650036683785,
          0.02641232575201761,
          0.02641232575201761,
          0.027879677182685254,
          0.027879677182685254,
          0.028613352898019074,
          0.028613352898019074,
          0.029347028613352897,
          0.029347028613352897,
          0.03008070432868672,
          0.03008070432868672,
          0.030814380044020543,
          0.030814380044020543,
          0.03154805575935436,
          0.03154805575935436,
          0.032281731474688186,
          0.032281731474688186,
          0.03301540719002201,
          0.03301540719002201,
          0.03374908290535583,
          0.03374908290535583,
          0.0359501100513573,
          0.0359501100513573,
          0.036683785766691124,
          0.036683785766691124,
          0.03888481291269259,
          0.03888481291269259,
          0.03961848862802641,
          0.03961848862802641,
          0.041085840058694055,
          0.041085840058694055,
          0.04181951577402788,
          0.04181951577402788,
          0.0425531914893617,
          0.0425531914893617,
          0.043286867204695524,
          0.043286867204695524,
          0.04402054292002935,
          0.04402054292002935,
          0.04548789435069699,
          0.04548789435069699,
          0.046221570066030816,
          0.046221570066030816,
          0.04695524578136464,
          0.04695524578136464,
          0.04768892149669846,
          0.04768892149669846,
          0.04842259721203228,
          0.04842259721203228,
          0.0491562729273661,
          0.0491562729273661,
          0.049889948642699924,
          0.049889948642699924,
          0.05062362435803375,
          0.05062362435803375,
          0.05135730007336757,
          0.05135730007336757,
          0.052090975788701394,
          0.052090975788701394,
          0.05355832721936904,
          0.05355832721936904,
          0.05429200293470286,
          0.05429200293470286,
          0.055025678650036686,
          0.055025678650036686,
          0.05575935436537051,
          0.05575935436537051,
          0.05649303008070433,
          0.05649303008070433,
          0.05942773294203962,
          0.05942773294203962,
          0.06016140865737344,
          0.06016140865737344,
          0.06089508437270726,
          0.06089508437270726,
          0.06236243580337491,
          0.06236243580337491,
          0.06382978723404255,
          0.06382978723404255,
          0.06603081438004402,
          0.06603081438004402,
          0.06676449009537784,
          0.06676449009537784,
          0.06969919295671313,
          0.06969919295671313,
          0.0719002201027146,
          0.0719002201027146,
          0.07336757153338225,
          0.07336757153338225,
          0.07850330154071901,
          0.07850330154071901,
          0.07997065297138664,
          0.07997065297138664,
          0.07997065297138664,
          0.08143800440205429,
          0.08143800440205429,
          0.08217168011738811,
          0.08217168011738811,
          0.08363903154805576,
          0.08363903154805576,
          0.08437270726338958,
          0.08437270726338958,
          0.08657373440939105,
          0.08657373440939105,
          0.08950843727072634,
          0.08950843727072634,
          0.09024211298606016,
          0.09024211298606016,
          0.09097578870139399,
          0.09097578870139399,
          0.09170946441672781,
          0.09170946441672781,
          0.0946441672780631,
          0.0946441672780631,
          0.09537784299339692,
          0.09537784299339692,
          0.09904622157006603,
          0.09904622157006603,
          0.10051357300073367,
          0.10051357300073367,
          0.10198092443140132,
          0.10198092443140132,
          0.10564930300807043,
          0.10564930300807043,
          0.1078503301540719,
          0.1078503301540719,
          0.1078503301540719,
          0.11005135730007337,
          0.11005135730007337,
          0.11151870873074102,
          0.11151870873074102,
          0.11151870873074102,
          0.11371973587674249,
          0.11371973587674249,
          0.11812179016874541,
          0.11812179016874541,
          0.11958914159941306,
          0.11958914159941306,
          0.12032281731474688,
          0.12032281731474688,
          0.1210564930300807,
          0.1210564930300807,
          0.12252384446074835,
          0.12252384446074835,
          0.123991195891416,
          0.123991195891416,
          0.12472487160674982,
          0.12472487160674982,
          0.12619222303741745,
          0.12619222303741745,
          0.12839325018341893,
          0.12839325018341893,
          0.1305942773294204,
          0.1305942773294204,
          0.13206162876008803,
          0.13206162876008803,
          0.13426265590608952,
          0.13426265590608952,
          0.13866471019809246,
          0.13866471019809246,
          0.1408657373440939,
          0.1408657373440939,
          0.144534115920763,
          0.144534115920763,
          0.14526779163609685,
          0.14526779163609685,
          0.1467351430667645,
          0.1467351430667645,
          0.1496698459280998,
          0.1496698459280998,
          0.1504035216434336,
          0.1504035216434336,
          0.15113719735876743,
          0.15113719735876743,
          0.15480557593543653,
          0.15480557593543653,
          0.15553925165077037,
          0.15553925165077037,
          0.15627292736610418,
          0.15627292736610418,
          0.15920763022743947,
          0.15920763022743947,
          0.15994130594277328,
          0.15994130594277328,
          0.16067498165810712,
          0.16067498165810712,
          0.16140865737344093,
          0.16140865737344093,
          0.1636096845194424,
          0.1636096845194424,
          0.16507703595011006,
          0.16507703595011006,
          0.1665443873807777,
          0.1665443873807777,
          0.1672780630961115,
          0.1672780630961115,
          0.16801173881144535,
          0.16801173881144535,
          0.16874541452677916,
          0.16874541452677916,
          0.169479090242113,
          0.169479090242113,
          0.17461482024944974,
          0.17461482024944974,
          0.17828319882611884,
          0.17828319882611884,
          0.18268525311812178,
          0.18268525311812178,
          0.18561995597945707,
          0.18561995597945707,
          0.1863536316947909,
          0.1863536316947909,
          0.1922230374174615,
          0.1922230374174615,
          0.19515774027879676,
          0.19515774027879676,
          0.19809244314013205,
          0.19809244314013205,
          0.1988261188554659,
          0.1988261188554659,
          0.20763022743947177,
          0.20763022743947177,
          0.20836390315480557,
          0.20836390315480557,
          0.2090975788701394,
          0.2090975788701394,
          0.21129860601614087,
          0.21129860601614087,
          0.21203228173147468,
          0.21203228173147468,
          0.2127659574468085,
          0.2127659574468085,
          0.21423330887747616,
          0.21423330887747616,
          0.2164343360234776,
          0.2164343360234776,
          0.21716801173881145,
          0.21716801173881145,
          0.21790168745414526,
          0.21790168745414526,
          0.22010271460014674,
          0.22010271460014674,
          0.2223037417461482,
          0.2223037417461482,
          0.2252384446074835,
          0.2252384446074835,
          0.22597212032281733,
          0.22597212032281733,
          0.22670579603815114,
          0.22670579603815114,
          0.22743947175348497,
          0.22743947175348497,
          0.22964049889948643,
          0.22964049889948643,
          0.23257520176082172,
          0.23257520176082172,
          0.23330887747615553,
          0.23330887747615553,
          0.23404255319148937,
          0.23404255319148937,
          0.23697725605282466,
          0.23697725605282466,
          0.23771093176815847,
          0.23771093176815847,
          0.2384446074834923,
          0.2384446074834923,
          0.2391782831988261,
          0.2391782831988261,
          0.24064563462949376,
          0.24064563462949376,
          0.2421129860601614,
          0.2421129860601614,
          0.24284666177549524,
          0.24284666177549524,
          0.24724871606749815,
          0.24724871606749815,
          0.25678650036683787,
          0.25678650036683787,
          0.2575201760821717,
          0.2575201760821717,
          0.2582538517975055,
          0.2582538517975055,
          0.26265590608950845,
          0.26265590608950845,
          0.2648569332355099,
          0.2648569332355099,
          0.26559060895084374,
          0.26559060895084374,
          0.26999266324284665,
          0.26999266324284665,
          0.2714600146735143,
          0.2714600146735143,
          0.2736610418195158,
          0.2736610418195158,
          0.2743947175348496,
          0.2743947175348496,
          0.2765957446808511,
          0.2765957446808511,
          0.2773294203961849,
          0.2773294203961849,
          0.2817314746881878,
          0.2817314746881878,
          0.28686720469552457,
          0.28686720469552457,
          0.2876008804108584,
          0.2876008804108584,
          0.2920029347028613,
          0.2920029347028613,
          0.293470286133529,
          0.293470286133529,
          0.2971386647101981,
          0.2971386647101981,
          0.2978723404255319,
          0.2978723404255319,
          0.31548055759354365,
          0.31548055759354365,
          0.32134996331621424,
          0.32134996331621424,
          0.3220836390315481,
          0.3220836390315481,
          0.32501834189288337,
          0.32501834189288337,
          0.326485693323551,
          0.326485693323551,
          0.32868672046955244,
          0.32868672046955244,
          0.33235509904622157,
          0.33235509904622157,
          0.334556126192223,
          0.334556126192223,
          0.3374908290535583,
          0.3374908290535583,
          0.33822450476889215,
          0.33822450476889215,
          0.338958180484226,
          0.338958180484226,
          0.33969185619955977,
          0.33969185619955977,
          0.3404255319148936,
          0.3404255319148936,
          0.34115920763022745,
          0.34115920763022745,
          0.3433602347762289,
          0.3433602347762289,
          0.34556126192223036,
          0.34556126192223036,
          0.3462949376375642,
          0.3462949376375642,
          0.3499633162142333,
          0.3499633162142333,
          0.35069699192956716,
          0.35069699192956716,
          0.3521643433602348,
          0.3521643433602348,
          0.35509904622157007,
          0.35509904622157007,
          0.3558327219369039,
          0.3558327219369039,
          0.3573000733675715,
          0.3573000733675715,
          0.36243580337490827,
          0.36243580337490827,
          0.3646368305209098,
          0.3646368305209098,
          0.36537050623624356,
          0.36537050623624356,
          0.3661041819515774,
          0.3661041819515774,
          0.3675715333822451,
          0.3675715333822451,
          0.3815113719735877,
          0.3815113719735877,
          0.3859134262655906,
          0.3859134262655906,
          0.3903154805575935,
          0.3903154805575935,
          0.3932501834189288,
          0.3932501834189288,
          0.39398385913426265,
          0.39398385913426265,
          0.3983859134262656,
          0.3983859134262656,
          0.4020542920029347,
          0.4020542920029347,
          0.4101247248716067,
          0.4101247248716067,
          0.41085840058694056,
          0.41085840058694056,
          0.41232575201760824,
          0.41232575201760824,
          0.41379310344827586,
          0.41379310344827586,
          0.41526045487894353,
          0.41526045487894353,
          0.41672780630961115,
          0.41672780630961115,
          0.417461482024945,
          0.417461482024945,
          0.4203961848862803,
          0.4203961848862803,
          0.42406456346294935,
          0.42406456346294935,
          0.425531914893617,
          0.425531914893617,
          0.4284666177549523,
          0.4284666177549523,
          0.42920029347028615,
          0.42920029347028615,
          0.42993396918561994,
          0.42993396918561994,
          0.4358033749082905,
          0.4358033749082905,
          0.44093910491562727,
          0.44093910491562727,
          0.4416727806309611,
          0.4416727806309611,
          0.44240645634629494,
          0.44240645634629494,
          0.44387380777696256,
          0.44387380777696256,
          0.4475421863536317,
          0.4475421863536317,
          0.44900953778429936,
          0.44900953778429936,
          0.45487894350696995,
          0.45487894350696995,
          0.45634629493763756,
          0.45634629493763756,
          0.4570799706529714,
          0.4570799706529714,
          0.4614820249449743,
          0.4614820249449743,
          0.4644167278063096,
          0.4644167278063096,
          0.46515040352164344,
          0.46515040352164344,
          0.4658840792369773,
          0.4658840792369773,
          0.4673514306676449,
          0.4673514306676449,
          0.4702861335289802,
          0.4702861335289802,
          0.4798239178283199,
          0.4798239178283199,
          0.4812912692589875,
          0.4812912692589875,
          0.48202494497432136,
          0.48202494497432136,
          0.4886280264123258,
          0.4886280264123258,
          0.49082905355832723,
          0.49082905355832723,
          0.491562729273661,
          0.491562729273661,
          0.4944974321349963,
          0.4944974321349963,
          0.4974321349963316,
          0.4974321349963316,
          0.4996331621423331,
          0.4996331621423331,
          0.5033015407190022,
          0.5033015407190022,
          0.5047688921496698,
          0.5047688921496698,
          0.5062362435803375,
          0.5062362435803375,
          0.5099046221570066,
          0.5099046221570066,
          0.5201760821716801,
          0.5201760821716801,
          0.5223771093176816,
          0.5223771093176816,
          0.5231107850330154,
          0.5231107850330154,
          0.5260454878943507,
          0.5260454878943507,
          0.532648569332355,
          0.532648569332355,
          0.5377842993396919,
          0.5377842993396919,
          0.5392516507703595,
          0.5392516507703595,
          0.5399853264856933,
          0.5399853264856933,
          0.5458547322083639,
          0.5458547322083639,
          0.5509904622157007,
          0.5509904622157007,
          0.5539251650770359,
          0.5539251650770359,
          0.5553925165077036,
          0.5553925165077036,
          0.5561261922230374,
          0.5561261922230374,
          0.5583272193690388,
          0.5583272193690388,
          0.57006603081438,
          0.57006603081438,
          0.5715333822450477,
          0.5715333822450477,
          0.5722670579603815,
          0.5722670579603815,
          0.5730007336757154,
          0.5730007336757154,
          0.5796038151137197,
          0.5796038151137197,
          0.5832721936903889,
          0.5832721936903889,
          0.586940572267058,
          0.586940572267058,
          0.5884079236977257,
          0.5884079236977257,
          0.5906089508437271,
          0.5906089508437271,
          0.5972120322817315,
          0.5972120322817315,
          0.5979457079970653,
          0.5979457079970653,
          0.6008804108584006,
          0.6008804108584006,
          0.6016140865737344,
          0.6016140865737344,
          0.6045487894350697,
          0.6045487894350697,
          0.6052824651504035,
          0.6052824651504035,
          0.607483492296405,
          0.607483492296405,
          0.6082171680117389,
          0.6082171680117389,
          0.6089508437270726,
          0.6089508437270726,
          0.6184886280264124,
          0.6184886280264124,
          0.6192223037417461,
          0.6192223037417461,
          0.6199559794570799,
          0.6199559794570799,
          0.6214233308877476,
          0.6214233308877476,
          0.6250917094644167,
          0.6250917094644167,
          0.6258253851797505,
          0.6258253851797505,
          0.6265590608950844,
          0.6265590608950844,
          0.6272927366104182,
          0.6272927366104182,
          0.6309611151870873,
          0.6309611151870873,
          0.6316947909024211,
          0.6316947909024211,
          0.632428466617755,
          0.632428466617755,
          0.6338958180484225,
          0.6338958180484225,
          0.6353631694790902,
          0.6353631694790902,
          0.6382978723404256,
          0.6382978723404256,
          0.6419662509170947,
          0.6419662509170947,
          0.6434336023477623,
          0.6434336023477623,
          0.6441672780630961,
          0.6441672780630961,
          0.6449009537784299,
          0.6449009537784299,
          0.6471019809244314,
          0.6471019809244314,
          0.6507703595011005,
          0.6507703595011005,
          0.652971386647102,
          0.652971386647102,
          0.6559060895084373,
          0.6559060895084373,
          0.6566397652237711,
          0.6566397652237711,
          0.6581071166544388,
          0.6581071166544388,
          0.6588407923697726,
          0.6588407923697726,
          0.6603081438004402,
          0.6603081438004402,
          0.6639765223771094,
          0.6639765223771094,
          0.6654438738077769,
          0.6654438738077769,
          0.6676449009537784,
          0.6676449009537784,
          0.6683785766691123,
          0.6683785766691123,
          0.6698459280997799,
          0.6698459280997799,
          0.6713132795304475,
          0.6713132795304475,
          0.673514306676449,
          0.673514306676449,
          0.6742479823917829,
          0.6742479823917829,
          0.6764490095377843,
          0.6764490095377843,
          0.6786500366837858,
          0.6786500366837858,
          0.6808510638297872,
          0.6808510638297872,
          0.6815847395451211,
          0.6815847395451211,
          0.6830520909757887,
          0.6830520909757887,
          0.6845194424064563,
          0.6845194424064563,
          0.685986793837124,
          0.685986793837124,
          0.6874541452677916,
          0.6874541452677916,
          0.6918561995597946,
          0.6918561995597946,
          0.6933235509904622,
          0.6933235509904622,
          0.6962582538517975,
          0.6962582538517975,
          0.6977256052824652,
          0.6977256052824652,
          0.7028613352898019,
          0.7028613352898019,
          0.7043286867204696,
          0.7043286867204696,
          0.7050623624358033,
          0.7050623624358033,
          0.7109317681584739,
          0.7109317681584739,
          0.7123991195891416,
          0.7123991195891416,
          0.7131327953044754,
          0.7131327953044754,
          0.7138664710198093,
          0.7138664710198093,
          0.7153338224504769,
          0.7153338224504769,
          0.7160674981658107,
          0.7160674981658107,
          0.7175348495964784,
          0.7175348495964784,
          0.7197358767424799,
          0.7197358767424799,
          0.7219369038884813,
          0.7219369038884813,
          0.723404255319149,
          0.723404255319149,
          0.7241379310344828,
          0.7241379310344828,
          0.727072633895818,
          0.727072633895818,
          0.7285399853264857,
          0.7285399853264857,
          0.7292736610418196,
          0.7292736610418196,
          0.7300073367571533,
          0.7300073367571533,
          0.7307410124724871,
          0.7307410124724871,
          0.7322083639031548,
          0.7322083639031548,
          0.7336757153338225,
          0.7336757153338225,
          0.7395451210564931,
          0.7395451210564931,
          0.7512839325018342,
          0.7512839325018342,
          0.7527512839325018,
          0.7527512839325018,
          0.7615553925165077,
          0.7615553925165077,
          0.7637564196625092,
          0.7637564196625092,
          0.764490095377843,
          0.764490095377843,
          0.7666911225238444,
          0.7666911225238444,
          0.7784299339691856,
          0.7784299339691856,
          0.7791636096845195,
          0.7791636096845195,
          0.7909024211298606,
          0.7909024211298606,
          0.7923697725605282,
          0.7923697725605282,
          0.7931034482758621,
          0.7931034482758621,
          0.7960381511371973,
          0.7960381511371973,
          0.8070432868672047,
          0.8070432868672047,
          0.8158473954512105,
          0.8158473954512105,
          0.8173147468818782,
          0.8173147468818782,
          0.818048422597212,
          0.818048422597212,
          0.8341892883345561,
          0.8341892883345561,
          0.8481291269258987,
          0.8481291269258987,
          0.8517975055025678,
          0.8517975055025678,
          0.8547322083639032,
          0.8547322083639032,
          0.8767424798239178,
          0.8767424798239178,
          0.8848129126925899,
          0.8848129126925899,
          0.8892149669845928,
          0.8892149669845928,
          0.8950843727072634,
          0.8950843727072634,
          0.8994864269992663,
          0.8994864269992663,
          0.9046221570066031,
          0.9046221570066031,
          0.9134262655906089,
          0.9134262655906089,
          0.9141599413059428,
          0.9141599413059428,
          0.9185619955979457,
          0.9185619955979457,
          0.9244314013206163,
          0.9244314013206163,
          0.925898752751284,
          0.925898752751284,
          0.9266324284666178,
          0.9266324284666178,
          0.9273661041819515,
          0.9273661041819515,
          0.9288334556126192,
          0.9288334556126192,
          0.9295671313279531,
          0.9295671313279531,
          0.9325018341892883,
          0.9325018341892883,
          0.9332355099046221,
          0.9332355099046221,
          0.933969185619956,
          0.933969185619956,
          0.9361702127659575,
          0.9361702127659575,
          0.9398385913426266,
          0.9398385913426266,
          0.9442406456346295,
          0.9442406456346295,
          0.9493763756419662,
          0.9493763756419662,
          0.9501100513573001,
          0.9501100513573001,
          0.9574468085106383,
          0.9574468085106383,
          0.9581804842259721,
          0.9581804842259721,
          0.9640498899486427,
          0.9640498899486427,
          0.9655172413793104,
          0.9655172413793104,
          0.9691856199559794,
          0.9691856199559794,
          0.9706529713866471,
          0.9706529713866471,
          0.971386647101981,
          0.971386647101981,
          0.9757887013939839,
          0.9757887013939839,
          0.9765223771093177,
          0.9765223771093177,
          0.9772560528246516,
          0.9772560528246516,
          0.979457079970653,
          0.979457079970653,
          0.9801907556859868,
          0.9801907556859868,
          0.9816581071166545,
          0.9816581071166545,
          0.9867938371239912,
          0.9867938371239912,
          0.9882611885546588,
          0.9882611885546588,
          0.9889948642699926,
          0.9889948642699926,
          0.9897285399853265,
          0.9897285399853265,
          0.9911958914159942,
          0.9911958914159942,
          0.9933969185619956,
          0.9933969185619956,
          0.9955979457079971,
          0.9955979457079971,
          0.9970652971386648,
          0.9970652971386648,
          0.9985326485693323,
          0.9985326485693323,
          1.0,
          1.0
        ],
        "tpr": [
          0.0,
          0.0007336757153338225,
          0.00586940572267058,
          0.00586940572267058,
          0.010271460014673514,
          0.010271460014673514,
          0.019075568598679385,
          0.019075568598679385,
          0.0359501100513573,
          0.0359501100513573,
          0.03741746148202495,
          0.03741746148202495,
          0.03888481291269259,
          0.03888481291269259,
          0.04035216434336023,
          0.04035216434336023,
          0.0425531914893617,
          0.0425531914893617,
          0.04768892149669846,
          0.04768892149669846,
          0.05135730007336757,
          0.05282465150403522,
          0.05355832721936904,
          0.05355832721936904,
          0.07263389581804842,
          0.07263389581804842,
          0.08363903154805576,
          0.0851063829787234,
          0.12252384446074835,
          0.12252384446074835,
          0.12325752017608217,
          0.12325752017608217,
          0.1438004402054292,
          0.1438004402054292,
          0.15187087307410124,
          0.15187087307410124,
          0.1533382245047689,
          0.1533382245047689,
          0.16214233308877476,
          0.16214233308877476,
          0.18782098312545856,
          0.18782098312545856,
          0.1922230374174615,
          0.1922230374174615,
          0.20102714600146734,
          0.20102714600146734,
          0.2223037417461482,
          0.22377109317681584,
          0.23257520176082172,
          0.23257520176082172,
          0.2384446074834923,
          0.2384446074834923,
          0.247982391782832,
          0.247982391782832,
          0.2553191489361702,
          0.2553191489361702,
          0.25678650036683787,
          0.25678650036683787,
          0.2765957446808511,
          0.2765957446808511,
          0.280997798972854,
          0.280997798972854,
          0.3330887747615554,
          0.3330887747615554,
          0.34115920763022745,
          0.34115920763022745,
          0.34262655906089506,
          0.34262655906089506,
          0.3433602347762289,
          0.3433602347762289,
          0.34849596478356565,
          0.34849596478356565,
          0.35803374908290536,
          0.359501100513573,
          0.36390315480557595,
          0.36390315480557595,
          0.3675715333822451,
          0.3675715333822451,
          0.3785766691122524,
          0.3785766691122524,
          0.3793103448275862,
          0.3793103448275862,
          0.38664710198092445,
          0.38664710198092445,
          0.39691856199559794,
          0.39691856199559794,
          0.40058694057226707,
          0.40058694057226707,
          0.4086573734409391,
          0.4086573734409391,
          0.40939104915627295,
          0.40939104915627295,
          0.4181951577402788,
          0.4181951577402788,
          0.4218635363169479,
          0.4218635363169479,
          0.4247982391782832,
          0.4247982391782832,
          0.4387380777696258,
          0.4387380777696258,
          0.4416727806309611,
          0.4416727806309611,
          0.44387380777696256,
          0.44387380777696256,
          0.4475421863536317,
          0.4475421863536317,
          0.4541452677916361,
          0.4541452677916361,
          0.4673514306676449,
          0.4673514306676449,
          0.46955245781364635,
          0.46955245781364635,
          0.4732208363903155,
          0.4732208363903155,
          0.4768892149669846,
          0.4768892149669846,
          0.4783565663976522,
          0.4783565663976522,
          0.4798239178283199,
          0.4798239178283199,
          0.4856933235509905,
          0.4856933235509905,
          0.491562729273661,
          0.491562729273661,
          0.4937637564196625,
          0.4937637564196625,
          0.4944974321349963,
          0.4944974321349963,
          0.49816581071166544,
          0.49816581071166544,
          0.5018341892883346,
          0.5018341892883346,
          0.504035216434336,
          0.504035216434336,
          0.5121056493030081,
          0.5121056493030081,
          0.5201760821716801,
          0.5201760821716801,
          0.520909757887014,
          0.520909757887014,
          0.5238444607483492,
          0.5238444607483492,
          0.5260454878943507,
          0.5260454878943507,
          0.5275128393250184,
          0.5275128393250184,
          0.5311812179016875,
          0.5311812179016875,
          0.532648569332355,
          0.532648569332355,
          0.5355832721936904,
          0.5355832721936904,
          0.5363169479090242,
          0.5363169479090242,
          0.5465884079236977,
          0.5465884079236977,
          0.5480557593543653,
          0.5480557593543653,
          0.5509904622157007,
          0.5509904622157007,
          0.5524578136463683,
          0.5524578136463683,
          0.5531914893617021,
          0.5531914893617021,
          0.5539251650770359,
          0.5539251650770359,
          0.5575935436537051,
          0.5575935436537051,
          0.5590608950843727,
          0.5590608950843727,
          0.5597945707997065,
          0.5597945707997065,
          0.5612619222303742,
          0.561995597945708,
          0.561995597945708,
          0.5627292736610419,
          0.5627292736610419,
          0.5641966250917094,
          0.5641966250917094,
          0.5656639765223771,
          0.5656639765223771,
          0.5671313279530448,
          0.5671313279530448,
          0.5678650036683786,
          0.5678650036683786,
          0.5730007336757154,
          0.5730007336757154,
          0.574468085106383,
          0.574468085106383,
          0.5752017608217168,
          0.5752017608217168,
          0.5759354365370506,
          0.5759354365370506,
          0.5774027879677183,
          0.5774027879677183,
          0.5788701393983859,
          0.5788701393983859,
          0.5832721936903889,
          0.5832721936903889,
          0.5847395451210565,
          0.5847395451210565,
          0.5876742479823918,
          0.5876742479823918,
          0.5884079236977257,
          0.5884079236977257,
          0.5891415994130594,
          0.5906089508437271,
          0.5906089508437271,
          0.5913426265590609,
          0.5913426265590609,
          0.5928099779897286,
          0.5935436537050623,
          0.5935436537050623,
          0.5957446808510638,
          0.5957446808510638,
          0.5964783565663977,
          0.5964783565663977,
          0.5972120322817315,
          0.5972120322817315,
          0.5986793837123991,
          0.5986793837123991,
          0.5994130594277329,
          0.5994130594277329,
          0.6001467351430667,
          0.6001467351430667,
          0.6016140865737344,
          0.6016140865737344,
          0.6038151137197358,
          0.6038151137197358,
          0.6067498165810712,
          0.6067498165810712,
          0.6089508437270726,
          0.6089508437270726,
          0.6096845194424064,
          0.6096845194424064,
          0.6111518708730741,
          0.6111518708730741,
          0.6118855465884079,
          0.6118855465884079,
          0.6133528980190756,
          0.6133528980190756,
          0.6148202494497432,
          0.6148202494497432,
          0.6170212765957447,
          0.6170212765957447,
          0.6177549523110785,
          0.6177549523110785,
          0.6214233308877476,
          0.6214233308877476,
          0.6228906823184153,
          0.6228906823184153,
          0.623624358033749,
          0.623624358033749,
          0.6250917094644167,
          0.6250917094644167,
          0.6258253851797505,
          0.6258253851797505,
          0.6272927366104182,
          0.6272927366104182,
          0.6280264123257521,
          0.6280264123257521,
          0.6316947909024211,
          0.6316947909024211,
          0.6331621423330888,
          0.6331621423330888,
          0.6338958180484225,
          0.6338958180484225,
          0.6346294937637564,
          0.6346294937637564,
          0.6353631694790902,
          0.6353631694790902,
          0.6360968451944241,
          0.6360968451944241,
          0.6368305209097579,
          0.6368305209097579,
          0.6375641966250917,
          0.6375641966250917,
          0.6382978723404256,
          0.6382978723404256,
          0.6390315480557593,
          0.6390315480557593,
          0.6397652237710931,
          0.6397652237710931,
          0.6412325752017608,
          0.6412325752017608,
          0.6419662509170947,
          0.6419662509170947,
          0.6434336023477623,
          0.6434336023477623,
          0.6441672780630961,
          0.6441672780630961,
          0.6456346294937637,
          0.6456346294937637,
          0.6463683052090976,
          0.6463683052090976,
          0.6478356566397653,
          0.6478356566397653,
          0.6493030080704328,
          0.6493030080704328,
          0.652971386647102,
          0.652971386647102,
          0.6537050623624358,
          0.6537050623624358,
          0.6544387380777696,
          0.6544387380777696,
          0.6551724137931034,
          0.6551724137931034,
          0.6566397652237711,
          0.6566397652237711,
          0.6603081438004402,
          0.6603081438004402,
          0.661041819515774,
          0.661041819515774,
          0.6632428466617755,
          0.6632428466617755,
          0.6639765223771094,
          0.6639765223771094,
          0.6764490095377843,
          0.6764490095377843,
          0.677916360968452,
          0.677916360968452,
          0.6793837123991195,
          0.6793837123991195,
          0.6808510638297872,
          0.6808510638297872,
          0.6815847395451211,
          0.6815847395451211,
          0.6823184152604549,
          0.6823184152604549,
          0.6830520909757887,
          0.6830520909757887,
          0.6845194424064563,
          0.6845194424064563,
          0.6874541452677916,
          0.6874541452677916,
          0.6881878209831255,
          0.6881878209831255,
          0.6896551724137931,
          0.6896551724137931,
          0.6903888481291269,
          0.6903888481291269,
          0.6911225238444607,
          0.6911225238444607,
          0.6918561995597946,
          0.6918561995597946,
          0.6925898752751284,
          0.6925898752751284,
          0.6955245781364637,
          0.6955245781364637,
          0.6962582538517975,
          0.6962582538517975,
          0.6977256052824652,
          0.6977256052824652,
          0.698459280997799,
          0.698459280997799,
          0.6991929567131328,
          0.6991929567131328,
          0.6999266324284666,
          0.6999266324284666,
          0.7013939838591343,
          0.7013939838591343,
          0.7028613352898019,
          0.7028613352898019,
          0.7035950110051358,
          0.7035950110051358,
          0.7043286867204696,
          0.7043286867204696,
          0.7057960381511372,
          0.7057960381511372,
          0.706529713866471,
          0.706529713866471,
          0.7087307410124725,
          0.7087307410124725,
          0.7116654438738078,
          0.7116654438738078,
          0.7123991195891416,
          0.7123991195891416,
          0.7131327953044754,
          0.7131327953044754,
          0.714600146735143,
          0.714600146735143,
          0.7160674981658107,
          0.7160674981658107,
          0.7168011738811445,
          0.7168011738811445,
          0.719002201027146,
          0.719002201027146,
          0.7204695524578136,
          0.7204695524578136,
          0.7212032281731474,
          0.7212032281731474,
          0.723404255319149,
          0.723404255319149,
          0.7263389581804842,
          0.7263389581804842,
          0.727072633895818,
          0.727072633895818,
          0.7278063096111519,
          0.7278063096111519,
          0.7292736610418196,
          0.7292736610418196,
          0.7300073367571533,
          0.7300073367571533,
          0.7307410124724871,
          0.7307410124724871,
          0.731474688187821,
          0.731474688187821,
          0.7322083639031548,
          0.7322083639031548,
          0.7329420396184886,
          0.7329420396184886,
          0.7336757153338225,
          0.7336757153338225,
          0.7351430667644901,
          0.7351430667644901,
          0.7358767424798239,
          0.7358767424798239,
          0.7366104181951577,
          0.7366104181951577,
          0.7388114453411592,
          0.7388114453411592,
          0.7395451210564931,
          0.7395451210564931,
          0.7402787967718268,
          0.7402787967718268,
          0.7417461482024945,
          0.7417461482024945,
          0.7424798239178283,
          0.7424798239178283,
          0.7432134996331622,
          0.7432134996331622,
          0.743947175348496,
          0.743947175348496,
          0.7468818782098312,
          0.7468818782098312,
          0.7476155539251651,
          0.7476155539251651,
          0.7483492296404989,
          0.7483492296404989,
          0.7490829053558328,
          0.7490829053558328,
          0.7505502567865003,
          0.7505502567865003,
          0.7512839325018342,
          0.7512839325018342,
          0.752017608217168,
          0.752017608217168,
          0.7542186353631695,
          0.7542186353631695,
          0.7564196625091709,
          0.7564196625091709,
          0.7571533382245048,
          0.7571533382245048,
          0.7578870139398386,
          0.7578870139398386,
          0.7586206896551724,
          0.7586206896551724,
          0.7593543653705063,
          0.7593543653705063,
          0.76008804108584,
          0.76008804108584,
          0.7608217168011738,
          0.7608217168011738,
          0.7630227439471754,
          0.7630227439471754,
          0.7652237710931769,
          0.7652237710931769,
          0.7666911225238444,
          0.7666911225238444,
          0.7681584739545121,
          0.7681584739545121,
          0.768892149669846,
          0.768892149669846,
          0.7696258253851798,
          0.7696258253851798,
          0.7703595011005135,
          0.7703595011005135,
          0.7710931768158474,
          0.7710931768158474,
          0.7718268525311812,
          0.7718268525311812,
          0.772560528246515,
          0.772560528246515,
          0.7732942039618489,
          0.7732942039618489,
          0.7740278796771827,
          0.7740278796771827,
          0.7747615553925165,
          0.7747615553925165,
          0.7762289068231841,
          0.7762289068231841,
          0.776962582538518,
          0.776962582538518,
          0.780630961115187,
          0.780630961115187,
          0.7820983125458547,
          0.7820983125458547,
          0.7835656639765224,
          0.7835656639765224,
          0.7857666911225238,
          0.7857666911225238,
          0.7865003668378576,
          0.7865003668378576,
          0.7872340425531915,
          0.7872340425531915,
          0.7879677182685253,
          0.7879677182685253,
          0.7909024211298606,
          0.7909024211298606,
          0.7916360968451944,
          0.7916360968451944,
          0.7923697725605282,
          0.7923697725605282,
          0.7931034482758621,
          0.7931034482758621,
          0.7938371239911959,
          0.7938371239911959,
          0.7945707997065297,
          0.7945707997065297,
          0.7953044754218636,
          0.7953044754218636,
          0.7967718268525312,
          0.7967718268525312,
          0.7982391782831988,
          0.7982391782831988,
          0.7989728539985327,
          0.7989728539985327,
          0.7997065297138665,
          0.7997065297138665,
          0.8011738811445341,
          0.8011738811445341,
          0.8026412325752018,
          0.8026412325752018,
          0.8033749082905356,
          0.8033749082905356,
          0.805575935436537,
          0.805575935436537,
          0.8063096111518708,
          0.8063096111518708,
          0.8070432868672047,
          0.8070432868672047,
          0.8077769625825385,
          0.8077769625825385,
          0.8114453411592076,
          0.8114453411592076,
          0.8121790168745414,
          0.8121790168745414,
          0.8136463683052091,
          0.8136463683052091,
          0.8143800440205429,
          0.8143800440205429,
          0.8151137197358768,
          0.8151137197358768,
          0.8158473954512105,
          0.8158473954512105,
          0.8165810711665444,
          0.8165810711665444,
          0.8173147468818782,
          0.8173147468818782,
          0.818048422597212,
          0.818048422597212,
          0.8187820983125459,
          0.8187820983125459,
          0.8202494497432135,
          0.8202494497432135,
          0.8209831254585473,
          0.8209831254585473,
          0.8217168011738811,
          0.8217168011738811,
          0.822450476889215,
          0.822450476889215,
          0.8231841526045488,
          0.8231841526045488,
          0.8246515040352165,
          0.8246515040352165,
          0.8253851797505503,
          0.8253851797505503,
          0.826118855465884,
          0.826118855465884,
          0.8268525311812179,
          0.8268525311812179,
          0.8275862068965517,
          0.8275862068965517,
          0.8283198826118855,
          0.8283198826118855,
          0.8297872340425532,
          0.8297872340425532,
          0.8305209097578871,
          0.8305209097578871,
          0.8312545854732208,
          0.8312545854732208,
          0.8319882611885546,
          0.8319882611885546,
          0.8341892883345561,
          0.8341892883345561,
          0.83492296404989,
          0.83492296404989,
          0.8356566397652238,
          0.8356566397652238,
          0.8371239911958914,
          0.8371239911958914,
          0.8393250183418929,
          0.8393250183418929,
          0.8400586940572267,
          0.8400586940572267,
          0.8407923697725606,
          0.8407923697725606,
          0.8415260454878943,
          0.8415260454878943,
          0.8422597212032281,
          0.8422597212032281,
          0.842993396918562,
          0.842993396918562,
          0.8444607483492297,
          0.8444607483492297,
          0.8459280997798972,
          0.8459280997798972,
          0.8473954512105649,
          0.8473954512105649,
          0.8495964783565664,
          0.8495964783565664,
          0.8503301540719003,
          0.8503301540719003,
          0.851063829787234,
          0.851063829787234,
          0.8525311812179017,
          0.8525311812179017,
          0.8532648569332355,
          0.8532648569332355,
          0.8539985326485693,
          0.8539985326485693,
          0.8561995597945709,
          0.8561995597945709,
          0.8569332355099046,
          0.8569332355099046,
          0.8576669112252384,
          0.8576669112252384,
          0.8584005869405723,
          0.8584005869405723,
          0.8591342626559061,
          0.8591342626559061,
          0.8598679383712399,
          0.8598679383712399,
          0.8606016140865738,
          0.8606016140865738,
          0.8613352898019075,
          0.8613352898019075,
          0.8620689655172413,
          0.8620689655172413,
          0.8628026412325752,
          0.8628026412325752,
          0.8642699926632429,
          0.8642699926632429,
          0.8650036683785767,
          0.8650036683785767,
          0.8657373440939105,
          0.8657373440939105,
          0.8664710198092443,
          0.8664710198092443,
          0.8672046955245781,
          0.8672046955245781,
          0.8679383712399119,
          0.8679383712399119,
          0.8686720469552458,
          0.8686720469552458,
          0.8701393983859135,
          0.8701393983859135,
          0.871606749816581,
          0.871606749816581,
          0.8723404255319149,
          0.8723404255319149,
          0.8730741012472487,
          0.8730741012472487,
          0.8738077769625825,
          0.8738077769625825,
          0.8745414526779164,
          0.8745414526779164,
          0.8760088041085841,
          0.8760088041085841,
          0.8767424798239178,
          0.8767424798239178,
          0.8782098312545855,
          0.8782098312545855,
          0.8789435069699193,
          0.8789435069699193,
          0.8796771826852531,
          0.8796771826852531,
          0.880410858400587,
          0.880410858400587,
          0.8818782098312545,
          0.8818782098312545,
          0.8826118855465884,
          0.8826118855465884,
          0.8833455612619222,
          0.8833455612619222,
          0.8848129126925899,
          0.8848129126925899,
          0.8855465884079237,
          0.8855465884079237,
          0.8870139398385913,
          0.8870139398385913,
          0.8877476155539251,
          0.8877476155539251,
          0.888481291269259,
          0.888481291269259,
          0.8899486426999267,
          0.8899486426999267,
          0.8906823184152605,
          0.8906823184152605,
          0.8921496698459281,
          0.8921496698459281,
          0.8928833455612619,
          0.8928833455612619,
          0.8950843727072634,
          0.8950843727072634,
          0.8958180484225972,
          0.8958180484225972,
          0.896551724137931,
          0.896551724137931,
          0.8987527512839325,
          0.8987527512839325,
          0.9002201027146002,
          0.9002201027146002,
          0.9016874541452677,
          0.9016874541452677,
          0.9024211298606016,
          0.9024211298606016,
          0.9031548055759354,
          0.9031548055759354,
          0.9046221570066031,
          0.9046221570066031,
          0.9060895084372708,
          0.9060895084372708,
          0.9068231841526045,
          0.9068231841526045,
          0.9075568598679383,
          0.9075568598679383,
          0.9097578870139399,
          0.9097578870139399,
          0.9104915627292737,
          0.9104915627292737,
          0.9112252384446075,
          0.9112252384446075,
          0.9119589141599413,
          0.9119589141599413,
          0.9126925898752751,
          0.9126925898752751,
          0.9134262655906089,
          0.9134262655906089,
          0.9141599413059428,
          0.9141599413059428,
          0.9148936170212766,
          0.9148936170212766,
          0.9163609684519443,
          0.9163609684519443,
          0.9178283198826119,
          0.9178283198826119,
          0.9192956713132795,
          0.9192956713132795,
          0.9200293470286134,
          0.9200293470286134,
          0.9207630227439472,
          0.9207630227439472,
          0.921496698459281,
          0.921496698459281,
          0.9222303741746148,
          0.9222303741746148,
          0.9229640498899486,
          0.9229640498899486,
          0.9236977256052825,
          0.9236977256052825,
          0.9266324284666178,
          0.9266324284666178,
          0.9273661041819515,
          0.9273661041819515,
          0.9280997798972854,
          0.9280997798972854,
          0.9295671313279531,
          0.9295671313279531,
          0.9303008070432869,
          0.9303008070432869,
          0.9310344827586207,
          0.9310344827586207,
          0.9317681584739546,
          0.9317681584739546,
          0.9325018341892883,
          0.9325018341892883,
          0.9332355099046221,
          0.9332355099046221,
          0.933969185619956,
          0.933969185619956,
          0.9354365370506236,
          0.9354365370506236,
          0.9361702127659575,
          0.9361702127659575,
          0.9369038884812912,
          0.9369038884812912,
          0.9376375641966251,
          0.9376375641966251,
          0.9383712399119589,
          0.9383712399119589,
          0.9391049156272927,
          0.9391049156272927,
          0.9398385913426266,
          0.9398385913426266,
          0.9405722670579604,
          0.9405722670579604,
          0.9413059427732942,
          0.9413059427732942,
          0.942039618488628,
          0.942039618488628,
          0.9427732942039618,
          0.9427732942039618,
          0.9435069699192957,
          0.9435069699192957,
          0.9442406456346295,
          0.9442406456346295,
          0.9449743213499633,
          0.9449743213499633,
          0.946441672780631,
          0.946441672780631,
          0.9471753484959647,
          0.9471753484959647,
          0.9486426999266324,
          0.9486426999266324,
          0.9493763756419662,
          0.9493763756419662,
          0.9501100513573001,
          0.9501100513573001,
          0.9508437270726339,
          0.9508437270726339,
          0.9515774027879678,
          0.9515774027879678,
          0.9523110785033015,
          0.9523110785033015,
          0.9530447542186353,
          0.9530447542186353,
          0.954512105649303,
          0.954512105649303,
          0.9559794570799707,
          0.9559794570799707,
          0.9567131327953045,
          0.9567131327953045,
          0.9574468085106383,
          0.9574468085106383,
          0.9581804842259721,
          0.9581804842259721,
          0.9589141599413059,
          0.9589141599413059,
          0.9596478356566398,
          0.9596478356566398,
          0.9603815113719736,
          0.9603815113719736,
          0.9611151870873074,
          0.9611151870873074,
          0.9618488628026413,
          0.9618488628026413,
          0.9640498899486427,
          0.9640498899486427,
          0.9647835656639765,
          0.9647835656639765,
          0.9662509170946442,
          0.9662509170946442,
          0.966984592809978,
          0.966984592809978,
          0.9684519442406456,
          0.9684519442406456,
          0.9699192956713133,
          0.9699192956713133,
          0.971386647101981,
          0.971386647101981,
          0.9721203228173148,
          0.9721203228173148,
          0.9735876742479824,
          0.9735876742479824,
          0.9743213499633162,
          0.9743213499633162,
          0.9787234042553191,
          0.9787234042553191,
          1.0
        ],
        "thresholds": [
          Infinity,
          0.530198872089386,
          0.5261234641075134,
          0.5259391665458679,
          0.5244855880737305,
          0.5242245197296143,
          0.5223749876022339,
          0.5223343968391418,
          0.5206307768821716,
          0.5205625891685486,
          0.5205298662185669,
          0.520431399345398,
          0.5203201770782471,
          0.5203006267547607,
          0.5199580788612366,
          0.5198088884353638,
          0.519550621509552,
          0.5193788409233093,
          0.5187691450119019,
          0.5187346339225769,
          0.5181458592414856,
          0.5181074142456055,
          0.5181061029434204,
          0.518039882183075,
          0.5175573825836182,
          0.5174877047538757,
          0.5171633362770081,
          0.5171574354171753,
          0.5166394114494324,
          0.5166260600090027,
          0.5166198015213013,
          0.5166115760803223,
          0.5164089202880859,
          0.5164005160331726,
          0.5162842273712158,
          0.5162805318832397,
          0.516218900680542,
          0.5162174105644226,
          0.516139030456543,
          0.5161376595497131,
          0.5158244371414185,
          0.5158184170722961,
          0.5157800316810608,
          0.515778660774231,
          0.5156549215316772,
          0.5156424641609192,
          0.5153673887252808,
          0.5153590440750122,
          0.5152644515037537,
          0.5152209997177124,
          0.5152047872543335,
          0.5152022242546082,
          0.515130341053009,
          0.5151215195655823,
          0.5150850415229797,
          0.515081524848938,
          0.5150794982910156,
          0.5150759816169739,
          0.5149422883987427,
          0.5149291753768921,
          0.5148984789848328,
          0.5148860216140747,
          0.5144814848899841,
          0.5144770741462708,
          0.5143796801567078,
          0.5143553018569946,
          0.5143525004386902,
          0.5143521428108215,
          0.5143511295318604,
          0.5143476128578186,
          0.5142896175384521,
          0.5142781734466553,
          0.5141728520393372,
          0.5141719579696655,
          0.5141388177871704,
          0.5141385197639465,
          0.514105498790741,
          0.5141032338142395,
          0.5139768719673157,
          0.5139682292938232,
          0.5139519572257996,
          0.513945460319519,
          0.5139126181602478,
          0.5138922929763794,
          0.51377272605896,
          0.5137691497802734,
          0.5137569904327393,
          0.513739824295044,
          0.5136746764183044,
          0.5136734843254089,
          0.5136604309082031,
          0.5136531591415405,
          0.513545572757721,
          0.5135440230369568,
          0.5135173201560974,
          0.5135148167610168,
          0.5134968161582947,
          0.5134950876235962,
          0.5133795738220215,
          0.5133612751960754,
          0.5133287310600281,
          0.5132872462272644,
          0.5132554173469543,
          0.5132370591163635,
          0.5131934881210327,
          0.5131263136863708,
          0.5130812525749207,
          0.5130804181098938,
          0.5129412412643433,
          0.5129004716873169,
          0.5128562450408936,
          0.5128558874130249,
          0.5128068923950195,
          0.5127964019775391,
          0.512742817401886,
          0.5127163529396057,
          0.5127063989639282,
          0.5126992464065552,
          0.5126931071281433,
          0.5126591324806213,
          0.5125556588172913,
          0.5125516057014465,
          0.5124590396881104,
          0.512395441532135,
          0.5123658180236816,
          0.5123592019081116,
          0.512353241443634,
          0.5123385787010193,
          0.5123063921928406,
          0.5122845768928528,
          0.5121986865997314,
          0.5121859312057495,
          0.5121546983718872,
          0.5121474862098694,
          0.5120242238044739,
          0.5120198130607605,
          0.5119074583053589,
          0.511885941028595,
          0.5118827819824219,
          0.5118796229362488,
          0.5118481516838074,
          0.5118103623390198,
          0.5117718577384949,
          0.5116921067237854,
          0.5116882920265198,
          0.5116668343544006,
          0.5116388201713562,
          0.5116354823112488,
          0.5116324424743652,
          0.5115536451339722,
          0.5115109086036682,
          0.5115029811859131,
          0.5114949345588684,
          0.5114887952804565,
          0.511436939239502,
          0.5114282965660095,
          0.5113968253135681,
          0.5113391876220703,
          0.5112149119377136,
          0.5111187100410461,
          0.5110836029052734,
          0.5110669136047363,
          0.5110210180282593,
          0.5109502673149109,
          0.5109473466873169,
          0.510932445526123,
          0.510875940322876,
          0.5108146667480469,
          0.510755181312561,
          0.5106048583984375,
          0.5105968117713928,
          0.510529100894928,
          0.5105260014533997,
          0.510515570640564,
          0.510489821434021,
          0.5104703307151794,
          0.5104668140411377,
          0.5104225277900696,
          0.5103641748428345,
          0.5103640556335449,
          0.5103626251220703,
          0.5103351473808289,
          0.5103152394294739,
          0.5103086829185486,
          0.5102425813674927,
          0.5100826621055603,
          0.5100734233856201,
          0.5100440979003906,
          0.5100418329238892,
          0.5100256204605103,
          0.5100192427635193,
          0.5099785327911377,
          0.5098966956138611,
          0.5098931789398193,
          0.5098481178283691,
          0.5098153948783875,
          0.5097004771232605,
          0.5095901489257812,
          0.5095500349998474,
          0.5095021724700928,
          0.5094765424728394,
          0.5094096660614014,
          0.5093127489089966,
          0.5092703700065613,
          0.5092234015464783,
          0.509216845035553,
          0.5092132687568665,
          0.5091630220413208,
          0.5091608762741089,
          0.509150505065918,
          0.5091477632522583,
          0.5091369152069092,
          0.5091172456741333,
          0.5090494751930237,
          0.5089195966720581,
          0.5089138150215149,
          0.5089007019996643,
          0.5088856816291809,
          0.5088702440261841,
          0.5088465809822083,
          0.5088103413581848,
          0.5087868571281433,
          0.5087277293205261,
          0.5087263584136963,
          0.5086889266967773,
          0.5086609125137329,
          0.5086479187011719,
          0.5086416602134705,
          0.5086151361465454,
          0.5085662007331848,
          0.508503258228302,
          0.5084850192070007,
          0.5084330439567566,
          0.5083930492401123,
          0.5083680748939514,
          0.508361279964447,
          0.508285403251648,
          0.508273720741272,
          0.5080741047859192,
          0.5080682039260864,
          0.5079903602600098,
          0.5079621076583862,
          0.507880449295044,
          0.5078698396682739,
          0.5078662037849426,
          0.50785231590271,
          0.5077875852584839,
          0.5077526569366455,
          0.5076895356178284,
          0.5076660513877869,
          0.5076431632041931,
          0.5076169371604919,
          0.5075984001159668,
          0.507527232170105,
          0.5073349475860596,
          0.5073293447494507,
          0.507309079170227,
          0.5072511434555054,
          0.507243275642395,
          0.5072425007820129,
          0.5070899724960327,
          0.5070646405220032,
          0.5070644617080688,
          0.5070315599441528,
          0.507031261920929,
          0.5070178508758545,
          0.5070163011550903,
          0.5070090293884277,
          0.506951093673706,
          0.5068950057029724,
          0.5067985653877258,
          0.5067800879478455,
          0.5067415833473206,
          0.50669264793396,
          0.5066854953765869,
          0.5066618323326111,
          0.5066455602645874,
          0.5065934658050537,
          0.5065768361091614,
          0.5065735578536987,
          0.50657057762146,
          0.5065634846687317,
          0.5064607262611389,
          0.5063706040382385,
          0.5062476396560669,
          0.5062419176101685,
          0.5060873627662659,
          0.5060519576072693,
          0.5059272050857544,
          0.5059214234352112,
          0.5059177875518799,
          0.5059096813201904,
          0.5056330561637878,
          0.5055981278419495,
          0.5054370760917664,
          0.5053988695144653,
          0.5052765607833862,
          0.5052393078804016,
          0.5052292346954346,
          0.505085289478302,
          0.5048481225967407,
          0.5047950744628906,
          0.5047431588172913,
          0.5047320127487183,
          0.5047097206115723,
          0.5046859979629517,
          0.5045323967933655,
          0.5044971108436584,
          0.5044936537742615,
          0.504422128200531,
          0.5044185519218445,
          0.5044081211090088,
          0.5043511390686035,
          0.5043288469314575,
          0.5042594075202942,
          0.504216194152832,
          0.5042101144790649,
          0.5041096806526184,
          0.504099428653717,
          0.5040687918663025,
          0.5039631128311157,
          0.503931999206543,
          0.5038613677024841,
          0.5038211345672607,
          0.5037989616394043,
          0.5037570595741272,
          0.5037243366241455,
          0.5037182569503784,
          0.5037146210670471,
          0.503704845905304,
          0.5037010908126831,
          0.5036641359329224,
          0.5036351084709167,
          0.503476619720459,
          0.5033975839614868,
          0.5033595561981201,
          0.5033359527587891,
          0.5033085346221924,
          0.503227174282074,
          0.5031881332397461,
          0.5031554102897644,
          0.5031430125236511,
          0.5031349062919617,
          0.503125786781311,
          0.5030897259712219,
          0.5030307173728943,
          0.5028796195983887,
          0.502822995185852,
          0.5026978254318237,
          0.5026680827140808,
          0.5025129914283752,
          0.5024392604827881,
          0.5023939609527588,
          0.5023365616798401,
          0.5020799040794373,
          0.5020615458488464,
          0.501579761505127,
          0.5015708208084106,
          0.5015378594398499,
          0.5014830231666565,
          0.5014498829841614,
          0.501441240310669,
          0.501187801361084,
          0.501168966293335,
          0.5011203289031982,
          0.5011051893234253,
          0.5010949969291687,
          0.5010460019111633,
          0.5009627342224121,
          0.5009594559669495,
          0.5009447932243347,
          0.5007773637771606,
          0.5007719993591309,
          0.5006749033927917,
          0.5006108283996582,
          0.5005452632904053,
          0.5004809498786926,
          0.5004560351371765,
          0.5004166960716248,
          0.5003780722618103,
          0.5001989006996155,
          0.5000725984573364,
          0.49985742568969727,
          0.49984636902809143,
          0.49982205033302307,
          0.49977487325668335,
          0.4994921088218689,
          0.4994155168533325,
          0.4992884695529938,
          0.4992866516113281,
          0.49913230538368225,
          0.4990677535533905,
          0.4990675151348114,
          0.4990231990814209,
          0.4984699487686157,
          0.49844110012054443,
          0.49814373254776,
          0.49813637137413025,
          0.49811092019081116,
          0.49810028076171875,
          0.4980553388595581,
          0.49804872274398804,
          0.4980246424674988,
          0.4979688227176666,
          0.497918963432312,
          0.4978226125240326,
          0.49776744842529297,
          0.4976980686187744,
          0.4976638853549957,
          0.497628778219223,
          0.49760007858276367,
          0.49756908416748047,
          0.49755674600601196,
          0.49747568368911743,
          0.4974592328071594,
          0.49744388461112976,
          0.49741503596305847,
          0.49738842248916626,
          0.49738290905952454,
          0.49724045395851135,
          0.49721869826316833,
          0.4972122311592102,
          0.49712109565734863,
          0.4970910847187042,
          0.4970056712627411,
          0.49696648120880127,
          0.49693554639816284,
          0.4969201385974884,
          0.4967711567878723,
          0.4967443346977234,
          0.4967224597930908,
          0.4967116415500641,
          0.4967009723186493,
          0.4966352581977844,
          0.4965524971485138,
          0.49648216366767883,
          0.4964747130870819,
          0.4964331388473511,
          0.4963901937007904,
          0.49631616473197937,
          0.49596717953681946,
          0.49592360854148865,
          0.49583104252815247,
          0.4957917332649231,
          0.4957602322101593,
          0.4956984221935272,
          0.49561232328414917,
          0.4956078827381134,
          0.4955672323703766,
          0.4955357611179352,
          0.4952458143234253,
          0.4952404201030731,
          0.4951426088809967,
          0.49514248967170715,
          0.4950811564922333,
          0.495023638010025,
          0.4948616921901703,
          0.494848370552063,
          0.4948321282863617,
          0.4948299527168274,
          0.49471473693847656,
          0.4947070777416229,
          0.4946140646934509,
          0.4945390522480011,
          0.49426034092903137,
          0.49424147605895996,
          0.49421924352645874,
          0.4941815733909607,
          0.4941069483757019,
          0.4939940273761749,
          0.4939636290073395,
          0.49396243691444397,
          0.4939271807670593,
          0.4938999116420746,
          0.49388691782951355,
          0.4938470721244812,
          0.493829607963562,
          0.493795245885849,
          0.4936835765838623,
          0.49368274211883545,
          0.49349185824394226,
          0.49347540736198425,
          0.4934456944465637,
          0.4934336841106415,
          0.4933956265449524,
          0.49338680505752563,
          0.49336087703704834,
          0.49332359433174133,
          0.49332231283187866,
          0.49326029419898987,
          0.4930977523326874,
          0.49306797981262207,
          0.4928257465362549,
          0.4927436113357544,
          0.49272921681404114,
          0.49266213178634644,
          0.49260467290878296,
          0.49251073598861694,
          0.4924662411212921,
          0.49234649538993835,
          0.49214065074920654,
          0.49213898181915283,
          0.4920836389064789,
          0.4920169413089752,
          0.4918385446071625,
          0.49181193113327026,
          0.49179062247276306,
          0.49163880944252014,
          0.4916151463985443,
          0.4915435314178467,
          0.49144068360328674,
          0.49141812324523926,
          0.49125421047210693,
          0.4911327362060547,
          0.49112361669540405,
          0.4910849630832672,
          0.49094313383102417,
          0.4908941984176636,
          0.49081361293792725,
          0.4908069372177124,
          0.4905940592288971,
          0.49053022265434265,
          0.4902538061141968,
          0.4901770353317261,
          0.4901285767555237,
          0.49012690782546997,
          0.4900699555873871,
          0.4900577664375305,
          0.4896388351917267,
          0.4895912706851959,
          0.48951050639152527,
          0.4894242584705353,
          0.48941391706466675,
          0.48940330743789673,
          0.48930785059928894,
          0.4892602264881134,
          0.489138126373291,
          0.48912298679351807,
          0.4890809953212738,
          0.4890729784965515,
          0.4889957308769226,
          0.4889260530471802,
          0.4889030158519745,
          0.4888390302658081,
          0.4888191521167755,
          0.4887942373752594,
          0.4886263906955719,
          0.4884992837905884,
          0.4879235327243805,
          0.4879007041454315,
          0.4878261089324951,
          0.48772746324539185,
          0.4877173900604248,
          0.48771584033966064,
          0.48752206563949585,
          0.4874856472015381,
          0.4871775209903717,
          0.4871424734592438,
          0.4869661331176758,
          0.48694151639938354,
          0.4869031310081482,
          0.4868945777416229,
          0.4868866205215454,
          0.4868069589138031,
          0.48654910922050476,
          0.48647236824035645,
          0.48633620142936707,
          0.48632413148880005,
          0.48624521493911743,
          0.48620399832725525,
          0.48610085248947144,
          0.4860976040363312,
          0.4860236942768097,
          0.4859597980976105,
          0.485870897769928,
          0.4858414828777313,
          0.4853864312171936,
          0.4853232204914093,
          0.4852689206600189,
          0.48526594042778015,
          0.4852117896080017,
          0.48518362641334534,
          0.48517122864723206,
          0.48515182733535767,
          0.4849412143230438,
          0.48483115434646606,
          0.4847416877746582,
          0.48473915457725525,
          0.4846704602241516,
          0.48466020822525024,
          0.48457202315330505,
          0.48450177907943726,
          0.48422765731811523,
          0.48406490683555603,
          0.4836220443248749,
          0.4836004972457886,
          0.48358845710754395,
          0.48352181911468506,
          0.4834546446800232,
          0.4834120273590088,
          0.4833366870880127,
          0.4832479655742645,
          0.4830995202064514,
          0.48308396339416504,
          0.4830593764781952,
          0.4830052852630615,
          0.482838898897171,
          0.4828210771083832,
          0.48276668787002563,
          0.4827653765678406,
          0.4826277494430542,
          0.4825517237186432,
          0.4816930294036865,
          0.48163050413131714,
          0.4816008508205414,
          0.4815444052219391,
          0.48150479793548584,
          0.48142167925834656,
          0.48136112093925476,
          0.4811919331550598,
          0.481034517288208,
          0.4809373617172241,
          0.48093634843826294,
          0.4809326231479645,
          0.48090699315071106,
          0.48078566789627075,
          0.48078417778015137,
          0.4807664453983307,
          0.4805636405944824,
          0.48034489154815674,
          0.4803444445133209,
          0.4802412688732147,
          0.48023656010627747,
          0.48005184531211853,
          0.4800260066986084,
          0.479964941740036,
          0.47989606857299805,
          0.4798584580421448,
          0.47959569096565247,
          0.4794924855232239,
          0.47938430309295654,
          0.4793713092803955,
          0.47925376892089844,
          0.479195237159729,
          0.4791879951953888,
          0.47915777564048767,
          0.4790858328342438,
          0.4790157377719879,
          0.47895193099975586,
          0.4789394438266754,
          0.47881415486335754,
          0.4787541627883911,
          0.4786771237850189,
          0.47857561707496643,
          0.4784989356994629,
          0.4784877300262451,
          0.47839561104774475,
          0.4783749282360077,
          0.47828876972198486,
          0.4781765341758728,
          0.47813278436660767,
          0.4781252443790436,
          0.47802528738975525,
          0.47792667150497437,
          0.47772324085235596,
          0.4777108430862427,
          0.47758346796035767,
          0.47757577896118164,
          0.4773847460746765,
          0.47735026478767395,
          0.4773423373699188,
          0.4772358536720276,
          0.4772052764892578,
          0.4771569073200226,
          0.4770015478134155,
          0.4769588112831116,
          0.4769226908683777,
          0.47681739926338196,
          0.4768066704273224,
          0.4767524302005768,
          0.4766383171081543,
          0.4764300286769867,
          0.47620925307273865,
          0.47620341181755066,
          0.47614696621894836,
          0.47614458203315735,
          0.4761209189891815,
          0.47608238458633423,
          0.47599101066589355,
          0.47580718994140625,
          0.47578948736190796,
          0.4757392108440399,
          0.4755156934261322,
          0.47550204396247864,
          0.47533220052719116,
          0.47526562213897705,
          0.47506123781204224,
          0.47501492500305176,
          0.47493496537208557,
          0.474882572889328,
          0.47444406151771545,
          0.47443774342536926,
          0.4743503928184509,
          0.47433486580848694,
          0.47420734167099,
          0.4740496575832367,
          0.47400158643722534,
          0.4739686846733093,
          0.4739395081996918,
          0.4739212393760681,
          0.4736976623535156,
          0.47369641065597534,
          0.47364377975463867,
          0.47342100739479065,
          0.47333791851997375,
          0.473337322473526,
          0.4733084738254547,
          0.47322729229927063,
          0.47317734360694885,
          0.47297021746635437,
          0.47274911403656006,
          0.47270968556404114,
          0.4726238548755646,
          0.47250956296920776,
          0.47243547439575195,
          0.4723905920982361,
          0.4721919596195221,
          0.47216251492500305,
          0.47213509678840637,
          0.47201335430145264,
          0.47195592522621155,
          0.47187381982803345,
          0.4717388451099396,
          0.47172603011131287,
          0.47162631154060364,
          0.47161865234375,
          0.4716007709503174,
          0.4714699387550354,
          0.47146403789520264,
          0.471451997756958,
          0.4714386761188507,
          0.47128817439079285,
          0.4712526500225067,
          0.47119325399398804,
          0.47109898924827576,
          0.4710613489151001,
          0.4706931412220001,
          0.47052696347236633,
          0.470131516456604,
          0.4700528681278229,
          0.46989819407463074,
          0.4698924124240875,
          0.46932369470596313,
          0.46923431754112244,
          0.4691292941570282,
          0.4689873456954956,
          0.4688914120197296,
          0.46865934133529663,
          0.46851325035095215,
          0.468440443277359,
          0.46769630908966064,
          0.4676841199398041,
          0.4676797389984131,
          0.4676707088947296,
          0.46732327342033386,
          0.4672820270061493,
          0.4671816825866699,
          0.4671687185764313,
          0.46713700890541077,
          0.46713027358055115,
          0.4670073390007019,
          0.4669128358364105,
          0.46647417545318604,
          0.4664497375488281,
          0.46604615449905396,
          0.466046005487442,
          0.46597710251808167,
          0.4659405052661896,
          0.4659101068973541,
          0.46590307354927063,
          0.4654895067214966,
          0.4654632806777954,
          0.4650753140449524,
          0.46507006883621216,
          0.4649549424648285,
          0.46489405632019043,
          0.4648156762123108,
          0.4647946357727051,
          0.46378427743911743,
          0.4637660086154938,
          0.46349257230758667,
          0.46340250968933105,
          0.4631025195121765,
          0.4630780518054962,
          0.46274518966674805,
          0.4627322852611542,
          0.4625205397605896,
          0.46245065331459045,
          0.462213933467865,
          0.46218135952949524,
          0.4616866111755371,
          0.4616737365722656,
          0.4616316258907318,
          0.461628794670105,
          0.4613208770751953,
          0.46131056547164917,
          0.46104076504707336,
          0.46100154519081116,
          0.4609485864639282,
          0.46092480421066284,
          0.46091675758361816,
          0.460883766412735,
          0.46080052852630615,
          0.4607342481613159,
          0.4605324864387512,
          0.460214763879776,
          0.4601882994174957,
          0.46018755435943604,
          0.46012794971466064,
          0.4600512981414795,
          0.4599885940551758,
          0.459987610578537,
          0.45994964241981506,
          0.4598333239555359,
          0.45970824360847473,
          0.45967498421669006,
          0.459285169839859,
          0.4592176377773285,
          0.45854172110557556,
          0.45852774381637573,
          0.45792290568351746,
          0.4578297734260559,
          0.45766258239746094,
          0.4576133191585541,
          0.45650964975357056,
          0.45646291971206665,
          0.4564453065395355,
          0.45615312457084656,
          0.4556131958961487,
          0.4554653465747833,
          0.4552914500236511,
          0.4552761912345886,
          0.4548622965812683,
          0.4547092020511627,
          0.45459049940109253,
          0.4545271396636963,
          0.4544433057308197,
          0.4543614685535431,
          0.4538319706916809,
          0.4537314772605896,
          0.45353418588638306,
          0.453513503074646,
          0.4534187614917755,
          0.45339787006378174,
          0.4532652795314789,
          0.45316144824028015,
          0.45315346121788025,
          0.45297694206237793,
          0.4526362419128418,
          0.4523971676826477,
          0.4518480896949768,
          0.45183759927749634,
          0.4516626000404358,
          0.4515664279460907,
          0.45152464509010315,
          0.4513017237186432,
          0.4510369598865509,
          0.45086491107940674,
          0.4504169821739197,
          0.4502094089984894,
          0.44993719458580017,
          0.44990602135658264,
          0.44966357946395874,
          0.44952771067619324,
          0.4487203359603882,
          0.44831740856170654,
          0.4479627013206482,
          0.4458793103694916,
          0.4447390139102936,
          0.408124178647995
        ]
      },
      "roc_auc": 0.775832732702505,
      "optimal_threshold": 0.5112149119377136
    },
    "ttt_model_metatasks": {
      "accuracy_mean": 0.9654,
      "accuracy_std": 0.003446737587922802,
      "precision_mean": 0.9654,
      "precision_std": 0.003446737587922802,
      "recall_mean": 0.9654,
      "recall_std": 0.003446737587922802,
      "macro_f1_mean": 0.9653938338772339,
      "macro_f1_std": 0.003449248925116082,
      "mcc_mean": 0.9311210909388411,
      "mcc_std": 0.006797532589395711,
      "confusion_matrix": [
        [
          1230,
          20
        ],
        [
          83,
          1167
        ]
      ],
      "roc_curve": {
        "fpr": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0008,
          0.0008,
          0.0008,
          0.0008,
          0.0008,
          0.0008,
          0.0008,
          0.0008,
          0.0016,
          0.0016,
          0.0024,
          0.0024,
          0.0048,
          0.0048,
          0.0064,
          0.0064,
          0.008,
          0.008,
          0.0096,
          0.0096,
          0.0112,
          0.0112,
          0.012,
          0.012,
          0.0128,
          0.0128,
          0.0136,
          0.0136,
          0.0144,
          0.0144,
          0.0152,
          0.0152,
          0.016,
          0.016,
          0.0168,
          0.0168,
          0.0176,
          0.0176,
          0.0184,
          0.0184,
          0.0224,
          0.0224,
          0.0232,
          0.0232,
          0.024,
          0.024,
          0.0256,
          0.0256,
          0.0272,
          0.0272,
          0.0296,
          0.0296,
          0.0312,
          0.0312,
          0.0328,
          0.0328,
          0.0336,
          0.0336,
          0.0344,
          0.0344,
          0.036,
          0.036,
          0.0368,
          0.0368,
          0.0384,
          0.0384,
          0.0392,
          0.0392,
          0.0408,
          0.0408,
          0.0448,
          0.0448,
          0.0488,
          0.0488,
          0.056,
          0.056,
          0.064,
          0.064,
          0.0648,
          0.0648,
          0.068,
          0.068,
          0.0728,
          0.0728,
          0.0792,
          0.0792,
          0.08,
          0.08,
          0.0816,
          0.0816,
          0.0848,
          0.0848,
          0.0936,
          0.0936,
          0.0944,
          0.0944,
          0.0952,
          0.0952,
          0.104,
          0.104,
          0.108,
          0.108,
          0.1168,
          0.1168,
          0.1248,
          0.1248,
          0.1264,
          0.1264,
          0.1296,
          0.1296,
          0.1432,
          0.1432,
          0.1472,
          0.1472,
          0.1504,
          0.1504,
          0.1664,
          0.1664,
          0.2096,
          0.2096,
          0.224,
          0.224,
          0.2312,
          0.2312,
          0.2832,
          0.2832,
          0.2984,
          0.2984,
          0.3072,
          0.3072,
          0.324,
          0.324,
          0.352,
          0.352,
          0.3744,
          0.3744,
          0.4136,
          0.4136,
          1.0
        ],
        "tpr": [
          0.0,
          0.0032,
          0.0112,
          0.0176,
          0.0184,
          0.0248,
          0.028,
          0.0288,
          0.032,
          0.0352,
          0.0368,
          0.0384,
          0.0408,
          0.0424,
          0.0448,
          0.0456,
          0.0488,
          0.0512,
          0.052,
          0.0536,
          0.0544,
          0.0592,
          0.0608,
          0.0632,
          0.0648,
          0.0712,
          0.0744,
          0.0888,
          0.0904,
          0.092,
          0.0952,
          0.1272,
          0.1288,
          0.1984,
          0.2016,
          0.2344,
          0.236,
          0.2608,
          0.2624,
          0.2928,
          0.2928,
          0.3048,
          0.3064,
          0.4096,
          0.4112,
          0.6024,
          0.604,
          0.7968,
          0.7968,
          0.8624,
          0.8624,
          0.888,
          0.888,
          0.8984,
          0.8984,
          0.904,
          0.904,
          0.9152,
          0.9152,
          0.916,
          0.916,
          0.9192,
          0.9192,
          0.9208,
          0.9208,
          0.9216,
          0.9216,
          0.9248,
          0.9248,
          0.928,
          0.928,
          0.9312,
          0.9312,
          0.936,
          0.936,
          0.9384,
          0.9384,
          0.94,
          0.94,
          0.9424,
          0.9424,
          0.9432,
          0.9432,
          0.944,
          0.944,
          0.9456,
          0.9456,
          0.9464,
          0.9464,
          0.9496,
          0.9496,
          0.952,
          0.952,
          0.9544,
          0.9544,
          0.956,
          0.956,
          0.9592,
          0.9592,
          0.9616,
          0.9616,
          0.964,
          0.964,
          0.9656,
          0.9656,
          0.9664,
          0.9664,
          0.9672,
          0.9672,
          0.968,
          0.968,
          0.9696,
          0.9696,
          0.9712,
          0.9712,
          0.9728,
          0.9728,
          0.9736,
          0.9736,
          0.9744,
          0.9744,
          0.9752,
          0.9752,
          0.9784,
          0.9784,
          0.9792,
          0.9792,
          0.98,
          0.98,
          0.9808,
          0.9808,
          0.9816,
          0.9816,
          0.9824,
          0.9824,
          0.9832,
          0.9832,
          0.984,
          0.984,
          0.9848,
          0.9848,
          0.9856,
          0.9856,
          0.9864,
          0.9864,
          0.9872,
          0.9872,
          0.988,
          0.988,
          0.9888,
          0.9888,
          0.9896,
          0.9896,
          0.9904,
          0.9904,
          0.9912,
          0.9912,
          0.992,
          0.992,
          0.9928,
          0.9928,
          0.9936,
          0.9936,
          0.9944,
          0.9944,
          0.9952,
          0.9952,
          0.996,
          0.996,
          0.9968,
          0.9968,
          0.9976,
          0.9976,
          0.9984,
          0.9984,
          0.9992,
          0.9992,
          1.0,
          1.0
        ],
        "thresholds": [
          Infinity,
          1.0,
          0.9999998807907104,
          0.9999997615814209,
          0.9999996423721313,
          0.9999994039535522,
          0.9999991655349731,
          0.9999990463256836,
          0.9999986886978149,
          0.9999985694885254,
          0.9999982118606567,
          0.9999980926513672,
          0.9999979734420776,
          0.9999978542327881,
          0.9999973773956299,
          0.9999972581863403,
          0.9999960660934448,
          0.9999959468841553,
          0.999995231628418,
          0.9999951124191284,
          0.9999948740005493,
          0.9999943971633911,
          0.9999940395355225,
          0.9999939203262329,
          0.9999932050704956,
          0.9999904632568359,
          0.9999898672103882,
          0.9999822378158569,
          0.9999818801879883,
          0.999981164932251,
          0.9999804496765137,
          0.9999494552612305,
          0.9999486207962036,
          0.999836802482605,
          0.9998360872268677,
          0.9997580647468567,
          0.9997571110725403,
          0.999681830406189,
          0.9996800422668457,
          0.999526858329773,
          0.9995249509811401,
          0.9994233846664429,
          0.9994208812713623,
          0.9983102083206177,
          0.9983038902282715,
          0.9874845147132874,
          0.987403392791748,
          0.8977482318878174,
          0.8970965147018433,
          0.7734946012496948,
          0.769747793674469,
          0.675998866558075,
          0.675032913684845,
          0.6495106220245361,
          0.6352903842926025,
          0.6123644113540649,
          0.6042134165763855,
          0.5587891936302185,
          0.5574150085449219,
          0.5550940036773682,
          0.5513606071472168,
          0.5391326546669006,
          0.5390134453773499,
          0.5358507037162781,
          0.5345150232315063,
          0.5335099697113037,
          0.5292192101478577,
          0.5237213969230652,
          0.5206894278526306,
          0.5150251388549805,
          0.5087935328483582,
          0.5040111541748047,
          0.5024402737617493,
          0.49667835235595703,
          0.4966091513633728,
          0.49466821551322937,
          0.49116531014442444,
          0.48882460594177246,
          0.4882799983024597,
          0.48215189576148987,
          0.4752757251262665,
          0.4747520685195923,
          0.47345981001853943,
          0.47335535287857056,
          0.47194036841392517,
          0.46606341004371643,
          0.463202565908432,
          0.4616529643535614,
          0.46043485403060913,
          0.4594053328037262,
          0.4589173495769501,
          0.4571681618690491,
          0.45689016580581665,
          0.4553113877773285,
          0.45458489656448364,
          0.45333120226860046,
          0.45265263319015503,
          0.4457647502422333,
          0.44209039211273193,
          0.4385663866996765,
          0.4345872700214386,
          0.43361642956733704,
          0.4326006770133972,
          0.4301445484161377,
          0.4292740523815155,
          0.42853790521621704,
          0.4284181296825409,
          0.42685872316360474,
          0.42593681812286377,
          0.4259245991706848,
          0.4162113070487976,
          0.4156329035758972,
          0.41088491678237915,
          0.4092380106449127,
          0.4004042446613312,
          0.39709821343421936,
          0.3876565098762512,
          0.3875439465045929,
          0.3874484896659851,
          0.38715192675590515,
          0.37702107429504395,
          0.3730541169643402,
          0.36711248755455017,
          0.36298519372940063,
          0.34882497787475586,
          0.3485400676727295,
          0.34786689281463623,
          0.346214234828949,
          0.33981168270111084,
          0.3395138680934906,
          0.3327387869358063,
          0.3318684995174408,
          0.3167838156223297,
          0.31615200638771057,
          0.31542766094207764,
          0.3145321309566498,
          0.31283435225486755,
          0.3117980659008026,
          0.3026692569255829,
          0.3025256097316742,
          0.2951279878616333,
          0.29339343309402466,
          0.2886933982372284,
          0.28865474462509155,
          0.27968689799308777,
          0.2789708971977234,
          0.27842438220977783,
          0.278389036655426,
          0.2756098806858063,
          0.27525636553764343,
          0.256388396024704,
          0.2558808922767639,
          0.25380170345306396,
          0.253620445728302,
          0.249265655875206,
          0.24921798706054688,
          0.2325751930475235,
          0.23104697465896606,
          0.18636414408683777,
          0.18603777885437012,
          0.171415776014328,
          0.17096349596977234,
          0.16147121787071228,
          0.16003935039043427,
          0.11576995998620987,
          0.11573060601949692,
          0.10353316366672516,
          0.10346954315900803,
          0.09868107736110687,
          0.09847643226385117,
          0.0917646735906601,
          0.09126320481300354,
          0.07904414087533951,
          0.07861001789569855,
          0.06695745885372162,
          0.0659528598189354,
          0.04792579635977745,
          0.047480229288339615,
          1.3288588895932207e-08
        ]
      },
      "roc_auc": 0.9934592000000001,
      "optimal_threshold": 0.4301445484161377
    },
    "improvement": {
      "accuracy_improvement": 0.1429999999999999,
      "precision_improvement": 0.1886267107126003,
      "recall_improvement": 0.11609790209790205,
      "f1_improvement": 0.14807951102742767,
      "mcc_improvement": 0,
      "zero_day_detection_improvement": 0.07199999999999995
    },
    "test_samples": 8178,
    "evaluated_samples": 8178,
    "meta_tasks_samples": 5000,
    "zero_day_samples": 4089,
    "timestamp": 1760153504.165497
  },
  "incentive_history": [
    {
      "round_number": 1,
      "total_rewards": 501,
      "timestamp": 1760153455.6767678
    }
  ],
  "client_addresses": {
    "client_1": "0xCD3a95b26EA98a04934CCf6C766f9406496CA986",
    "client_2": "0x32cE285CF96cf83226552A9c3427Bd58c0A9AccD",
    "client_3": "0x8EbA3b47c80a5E31b4Ea6fED4d5De8ebc93B8d6f"
  },
  "timestamp": 1760153510.871304
}